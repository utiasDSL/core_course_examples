{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68a9444e",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 5.1: Model Predictive Control**\n",
    "\n",
    "\n",
    "\n",
    "In this chapter, we introduce model predictive control (MPC), a powerful and widely used framework for solving constrained optimal control problems in both linear and nonlinear systems. Unlike ILQR, which computes a complete open-loop policy offline, MPC solves a finite-horizon optimal control problem (OCP) at each time step, using the current system state as the initial condition. Only the first control input of the optimized sequence is applied before the horizon shifts forward and the process repeats — a concept known as receding horizon control.\n",
    "\n",
    "We begin by motivating the use of MPC in comparison to open-loop OCP formulations, emphasizing its ability to incorporate feedback. The first part of this chapter focuses on linear MPC (LMPC), where we demonstrate how to reformulate the control problem into a quadratic program (QP), implement the controller, and explore how the horizon and the terminal components affect the LMPC performance.\n",
    "\n",
    "In the second part, we extend this formulation to nonlinear MPC (NMPC) by leveraging state-of-the-art OCP solvers such as Acados, and walk through the key components of an NMPC pipeline: from solver setup to closed-loop control. Finally, we provide a comparative study between NMPC and classical controllers, like ILQR, highlighting MPC's advantage in handling constraints. We conclude by discussing how key hyperparameters, particularly the prediction horizon, affect the performance and computational cost of MPC.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 5.1 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 1 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Open-Loop</td>\n",
    "    <td>Example 1: Cost Designs in Optimal Control</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 2: Open-Loop Limitations</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Linear MPC</td>\n",
    "    <td>Task 1: Linear MPC Implementation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 2: Simulation and Visualization</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 3: Horizon and Terminal Components</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 4: LMPC vs. LQR</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 3 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"4\">Nonlinear MPC</td>\n",
    "    <td>Task 1: Solver Configuration</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 2: Control Loop Definition</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 3: Simulation and Visualization</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Task 4: NMPC vs. ILQR</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"1\">Hyperparameter Selection</td>\n",
    "    <td>Example: Prediction Horizon</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1e8fa-a5c2-4219-a5d0-d6cc0ade876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import casadi as ca\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils.env import *\n",
    "from utils.controller import *\n",
    "from utils.simulator import *\n",
    "from ex2_LQR.lqr_utils import *\n",
    "from ex4_iLQR.ilqr_utils import *\n",
    "from ex5_MPC.mpc_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e96ed6",
   "metadata": {},
   "source": [
    "### **Problem Setup:**\n",
    "\n",
    "- Task: starting from given initial position $p_0$, reach a given target position $p_T$ (stabilization)\n",
    "\n",
    "- Slope profile (height $h$ with reference to horizontal displacement $p$):  \n",
    "   - case 1: zero slope (linear case), $h(p) = c$\n",
    "   - case 2: constant slope (linear case), $h(p) = \\frac{\\pi}{18} \\cdot p$\n",
    "   - case 3: varying slope for small disturbances (nonlinear case), $h(p) = k \\cdot \\cos(18 p)$\n",
    "   - case 4: varying slope for under actuated case (nonlinear case), $h(p) = \\begin{cases} k \\cdot \\sin(3 p), & p \\in [- \\frac{\\pi}{2}, \\frac{\\pi}{6}] \\\\ k, & p \\in (-\\infty, -\\frac{\\pi}{2}) \\cup (\\frac{\\pi}{6}, \\infty) \\end{cases}$\n",
    "\n",
    "- System dynamics of 1d mountain car model (in state space representation): \n",
    "   - state vector $\\boldsymbol{x} = [p, v]^T$\n",
    "   - input vector $u$\n",
    "   - system dynamics:\n",
    "   \\begin{align*}\n",
    "     \\begin{bmatrix} \\dot{p} \\\\ \\dot{v} \\end{bmatrix} = \\begin{bmatrix} v \\\\ - g \\sin(\\theta) \\cos(\\theta) \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\cos(\\theta)  \\end{bmatrix} u\n",
    "   \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd621bc5",
   "metadata": {},
   "source": [
    "### **Preparation: Mountain Car Environment and the System Dynamics**\n",
    "\n",
    "In the previous exercise, we demonstrated how to define a symbolic function using CasADi, including the definition of the mountain profile as a function of $p$, deriving the conversion formulas between the slope profile $h(p)$ and the inclination angle $\\theta(p)$, and establishing the system's dynamics. These formulas have already been integrated into the class `Env` and `Dynamics`. In this chapter, we will specify the arguments and instantiate these classes directly to utilize their functionalities.\n",
    "\n",
    "- Parameters in the task:  \n",
    "\n",
    "   - case: 1 (linear case)\n",
    "   \n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.6, 0.0]^T$\n",
    "\n",
    "   - input constraints: $ \\mathcal{U} \\in [-1.0, 1.0]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d782c95-de15-49ec-a999-37ee410d8a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define profile of slope, the initial / target state\n",
    "case = 1 # 1 or 2 or 3 or 4\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the constraints for input (potentially used)\n",
    "input_lbs = -1.0\n",
    "input_ubs = 1.0\n",
    "\n",
    "# Case 1: unconstrained case\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "dynamics = Dynamics(env)\n",
    "\n",
    "# Case 2: constrained case\n",
    "env_constr = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_constr = Dynamics(env_constr)\n",
    "\n",
    "env.test_env() # show shape of slope (left side) and theta curve (right side) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ef8db",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part (a): Open-Loop**\n",
    "\n",
    "We first consider a simple constrained open-loop optimal control problem (OCP), which is solved offline over the entire time horizon by formulating and optimizing a quadratic (or more generally, nonlinear) programming problem to obtain the input sequence that minimizes a given objective function:\n",
    "\n",
    "$$\n",
    "J = g_N(\\boldsymbol{x}_{N}) + \\sum_{k=0}^{N-1} g_k(\\boldsymbol{x}_{k}, \\boldsymbol{u}_k)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k, \\quad \\forall k \\in \\{0, \\dots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x}_k \\in \\mathcal{X}, \\quad \\forall k \\in \\{0, \\dots, N\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{u}_k \\in \\mathcal{U}, \\quad \\forall k \\in \\{0, \\dots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{0}} = \\boldsymbol{\\overline{x}_{0}}\n",
    "$$\n",
    "\n",
    "By fixing the algorithmic hyperparameters and applying the optimal input sequence obtained from solving the OCP to the simulation environment, we obtain the following simulation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26460f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q  # terminal cost (here the same as the stage cost)\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 8 # time length of simulation\n",
    "\n",
    "# Compute horizon for open-loop optimal controller (full horizon)\n",
    "N = freq * t_terminal # here 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b120a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the OCP controller\n",
    "controller_ocp_2n = LinearOCPController(env_constr, dynamics_constr, Q, R, Qf, freq, N, name='OCP-2-norm')\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ocp_2n = Simulator(dynamics_constr, controller_ocp_2n, env_constr, 1/freq, t_terminal)\n",
    "simulator_ocp_2n.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ocp_2n = Visualizer(simulator_ocp_2n)\n",
    "visualizer_ocp_2n.display_plots()\n",
    "visualizer_ocp_2n.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54547f3",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 1: Cost Designs in Optimal Control**  \n",
    "\n",
    "Apart from the standard quadratic cost (2-norm), one can also use different norms, such as the 1-norm or infinity-norm, to achieve different control characteristics. These alternative cost designs lead to different system behavior and are particularly useful when specific structural or robustness properties are desired. \n",
    "\n",
    "For example, the following shows a design of an MPC cost function based on the 1-norm:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{u_0, u_1, \\dots, u_{N-1}} && \\sum_{k=0}^{N-1} |u_k| \\\\\n",
    "& \\text{subject to} \\quad \n",
    "&& x_{k+1} = x_k + u_k, \\quad \\forall k \\in \\{0, 1, \\dots, N-1\\}, \\\\\n",
    "&&& x_0 = \\bar{x}_0,\\\\\n",
    "&&& \\boxed{x_N = x_g,} \\\\\n",
    "&&& \\boldsymbol{x}_k \\in \\mathcal{X}, \\quad \\forall k \\in \\{0, \\dots, N\\},\\\\\n",
    "&&& \\boldsymbol{u}_k \\in \\mathcal{U}, \\quad \\forall k \\in \\{0, \\dots, N-1\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "*Note 1: the cost function here is not directly defined in terms of the state $\\boldsymbol{x}$. Therefore, to ensure that the system state remains close to the desired target, we impose a terminal constraint that explicitly forces the final state in each MPC horizon to be equal to the target state.*\n",
    "\n",
    "*Note 2: recall from the course that we have shown how to reformulate the OCP problem with 1-norm cost into a QP problem. But here there's no need to do such reformulation because the solver we used (Acados, shown in Section 3) could do that internally for us*\n",
    "<br>\n",
    "\n",
    "Here, we directly provide an implementation of the controller. You can simply instantiate it and run the simulation to observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad8ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1-norm cost design\n",
    "\n",
    "# Define the OCP controller\n",
    "controller_ocp_1n = LinearOCP1NormController(env_constr, dynamics_constr, freq, N)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ocp_1n = Simulator(dynamics_constr, controller_ocp_1n, env_constr, 1/freq, t_terminal)\n",
    "simulator_ocp_1n.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ocp_1n = Visualizer(simulator_ocp_1n)\n",
    "visualizer_ocp_1n.display_plots()\n",
    "visualizer_ocp_1n.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e7c964",
   "metadata": {},
   "source": [
    "Another popular choice is the infinity norm, as shown in the following form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{u_0, u_1, \\dots, u_{N-1}} && \\max_{k \\in \\{0, \\dots, N-1\\}} |u_k| \\\\\n",
    "& \\text{subject to} \\quad \n",
    "&& x_{k+1} = x_k + u_k, \\quad \\forall k \\in \\{0, 1, \\dots, N-1\\}, \\\\\n",
    "&&& x_0 = \\bar{x}_0,\\\\\n",
    "&&& x_N = x_g,  \\\\\n",
    "&&& \\boldsymbol{x}_k \\in \\mathcal{X}, \\quad \\forall k \\in \\{0, \\dots, N\\},\\\\\n",
    "&&& \\boldsymbol{u}_k \\in \\mathcal{U}, \\quad \\forall k \\in \\{0, \\dots, N-1\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since most solvers cannot directly handle non-smooth objectives involving the maximum operator (such as $\\max_{k} |u_k|$), we reformulate the problem by introducing an auxiliary variable $q$, resulting in an equivalent linear program. This transformation (also known as epigraph reformulation) makes the problem solver-compatible while preserving the original optimization:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\min_{q, u_0, \\dots, u_{N-1}} && \\boxed{q} \\\\\n",
    "& \\text{subject to} \\quad \n",
    "&& x_{k+1} = x_k + u_k, \\quad \\forall k \\in \\{0, \\dots, N-1\\}, \\\\\n",
    "&&& x_0 = \\bar{x}_0, \\\\\n",
    "&&& x_N = x_g, \\\\\n",
    "&&& \\boxed{-q \\le u_k \\le q, \\quad \\forall k \\in \\{0, \\dots, N-1\\},}  \\\\\n",
    "&&& \\boldsymbol{x}_k \\in \\mathcal{X}, \\quad \\forall k \\in \\{0, \\dots, N\\},\\\\\n",
    "&&& \\boldsymbol{u}_k \\in \\mathcal{U}, \\quad \\forall k \\in \\{0, \\dots, N-1\\}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Similar to the 1-norm class, you can simply instantiate the inf-norm class and run the simulation to observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522acf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inf-norm cost design\n",
    "N = freq * t_terminal # here 160\n",
    "\n",
    "# Define the OCP controller\n",
    "controller_ocp_infn = LinearOCPInfNormController(env, dynamics, freq, N)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ocp_infn = Simulator(dynamics, controller_ocp_infn, env, 1/freq, t_terminal)\n",
    "simulator_ocp_infn.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ocp_infn = Visualizer(simulator_ocp_infn)\n",
    "visualizer_ocp_infn.display_plots()\n",
    "visualizer_ocp_infn.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2237d3",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "The following figure illustrates how different norm-based cost functions (specifically, the $l_1$-norm, $l_2$-norm, and $l_{\\infty}$-norm) lead to distinct control behaviors, even under the same initial and terminal conditions, dynamics, and input constraints.\n",
    "\n",
    "From the perspective of optimal control, the choice of the norm directly shapes the optimal control policy:\n",
    "\n",
    "- The $l_1$-norm penalizes the absolute sum of control inputs, which **promotes sparsity** in the solution. As shown in the input plot, the controller generates long intervals of zero or constant control, applying input only when necessary. This is often desirable when **minimizing actuator usage or energy consumption**.\n",
    "\n",
    "- The $l_2$-norm, being quadratic and smooth, encourages **smooth and distributed control effort over time**. This leads to continuous acceleration profiles and naturally shaped velocity responses. It is commonly used due to its strong mathematical properties (e.g., uniqueness, differentiability) and **robustness to noise**.\n",
    "\n",
    "- The $l_{\\infty}$-norm minimizes the maximum absolute control effort across the horizon, which often results in **bang-bang-like control**—i.e., switching between the upper and lower bounds. This is advantageous **when enforcing strict limits on peak actuator usage**, but can introduce chattering or less smooth behavior.\n",
    "\n",
    "This example highlights that the norm selected in cost design is not just a mathematical formality, but will significantly affect the qualitative behavior of the resulting control policy. **The smoothness and sparsity can be tailored by choosing an appropriate norm to reflect the control objective.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94951ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison under different norm-based cost designs\n",
    "visualizer_ocp_1n.display_contrast_plots(simulator_ocp_2n, simulator_ocp_infn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107d6ec",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 2: Open-Loop Limitations** \n",
    "\n",
    "One advantage of the open-loop algorithm is that it allows for offline pre-computation. However, it also has several limitations:\n",
    "\n",
    "- The optimization must be performed over the entire simulation horizon. For example, with a simulation duration of $8$ seconds and a control frequency of $20$ Hz, the problem involves $160$ time steps, resulting in a relatively high computational cost;\n",
    "\n",
    "- Open-loop policies are inherently less robust to noise and model-uncertainty when deployed on a real system. Any disturbance during execution can significantly degrade the performance of the control policy.\n",
    "\n",
    "In offline computation, we can afford nearly unlimited computational resources and time, so the first drawback is often negligible in practice. However, the second limitation is far more critical: during actual controller deployment, model mismatch and unavoidable disturbances are almost always present.\n",
    "\n",
    "To illustrate how such uncertainties can degrade the performance of open-loop control, we present a representative example that highlights the vulnerability of open-loop policies under real-world deviations from the nominal model:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1\\) Use the dynamics from the **flat case (case 1)** to design the open-loop optimal controller: the discrete dynamics model used in the controller is derived from the cart dynamics on a flat ground, which is the same as `controller_ocp_2n`.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2\\) Use the **bumpy case (case 3)** as the simulation environment: the mountain profile is designed as a sinusoidal wave that extends along the horizontal axis. By setting a relatively small amplitude, we aim to simulate minor perturbations commonly encountered in real-world scenarios.\n",
    "\n",
    "By designing the controller and performing the simulation in different environments, we are able to simulate disturbances that are unknown at the time of controller design, as would be the case in the real world. This allows us to assess the robustness of the controller under model mismatch and unexpected variations. Please run the simulation and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7556ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinear case number\n",
    "case_disturb = 3\n",
    "\n",
    "# Instantiate class 'Env'\n",
    "env_disturb = Env(case_disturb, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env_disturb.test_env()\n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_disturb = Dynamics(env_disturb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c48d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ocp_disturb = Simulator(dynamics_disturb, controller_ocp_2n, env_disturb, 1/freq, t_terminal)\n",
    "simulator_ocp_disturb.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ocp_disturb = Visualizer(simulator_ocp_disturb)\n",
    "visualizer_ocp_disturb.display_plots()\n",
    "visualizer_ocp_disturb.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42337b",
   "metadata": {},
   "source": [
    "From the simulation results, we can observe that the open-loop controller produces **exactly the same control input sequence as before**. However, due to the presence of persistent small disturbances in the environment, the system behavior deviates from the nominal trajectory. As a result, the controller fails to drive the cart to the target state, highlighting the vulnerability of open-loop control under real-world uncertainties.\n",
    "\n",
    "So, is there a way to introduce a feedback mechanism into optimal control? Suppose we could :\n",
    "\n",
    "- Re-solve an open-loop optimal control problem at every time step, using the current state as the new initial condition. $\\boldsymbol{x_{k|k}} = \\boldsymbol{\\overline{x}_{k}}$\n",
    "\n",
    "- Furthermore, instead of solving the problem over the entire time horizon, we could consider only a shorter prediction horizon into the future $N_{mpc} \\ll N_{full}$. This significantly reduces the computational burden, making online optimization feasible.\n",
    "\n",
    "These ideas form the core of model predictive control (MPC), also known as receding horizon control, which a widely-used approach that solves a constrained optimization problem at each time step. The MPC problem can be formulated as follows:\n",
    "\n",
    "$$\n",
    "J_k^*(\\boldsymbol{x_k}) = \\min_{u_{k|k}, \\ldots, u_{k+N-1|k}} \n",
    "g_N\\left( \\boldsymbol{x_{k+N|k}} \\right) + \\sum_{i=0}^{N-1} g_i\\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i+1|k}} = \\boldsymbol{A} \\boldsymbol{x_{k+i|k}} + \\boldsymbol{B} \\boldsymbol{u_{k+i|k}}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i|k}} \\in \\mathcal{X}, \\quad \\forall i \\in \\{0, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_{k+i|k} \\in \\mathcal{U}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k|k}} = \\boldsymbol{\\overline{x}_{k}}\n",
    "$$\n",
    "\n",
    "*Note that: It is important to distinguish the notation used in the OCP formulation from that in the MPC context. In the OCP problem, the index $k$ denotes a discrete time step along the entire simulation horizon. Since the OCP is solved once offline, this global time index is consistent throughout the problem. In contrast, MPC involves solving an optimization problem at each time step $k$ of the simulation. Within each individual MPC problem, a separate index $i$ is introduced to represent the steps along the prediction horizon at time $k$. This separation of $k$ (global time step) and $i$ (local horizon index) helps clarify the distinction between the simulation timeline and the prediction horizon.*\n",
    "\n",
    "<br>\n",
    "\n",
    "Based on a simple MPC implementation that we have prepared for you, you can initialize the solver using the same set of hyperparameters as in the previous OCP problem (except for the prediction horizon $N$, which needs to be specified separately), and run simulations to observe the resulting behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the prediction horizon (could be much smaller than before)\n",
    "N=20\n",
    "\n",
    "# Define the MPC controller\n",
    "controller_mpc = MPCController(env, dynamics, Q, R, Qf, freq, N, name='MPC_0', verbose=None)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_disturb = Simulator(dynamics_disturb, controller_mpc, env_disturb, 1/freq, t_terminal)\n",
    "simulator_mpc_disturb.run_simulation()\n",
    "\n",
    "print(f\"Average computation time per step: {(time.time()-start_time) / (freq * t_terminal)}\")\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_disturb = Visualizer(simulator_mpc_disturb)\n",
    "visualizer_mpc_disturb.display_plots()\n",
    "visualizer_mpc_disturb.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6153ef",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "By comparing the simulation results of open-loop OCP and MPC under environment with uncertainties, we can draw the following conclusions:\n",
    "\n",
    "- The computation time required for each MPC update is significantly shorter than that of the full OCP (e.g., the average computation time is only $0.1$ ms for $N=20$), making MPC particularly well-suited for computation online.\n",
    "\n",
    "- By re-solving the optimal control problem at each time step based on the current state (i.e., through online computation), we successfully create a feedback law. \n",
    "\n",
    "In summary, MPC provides a close approximation to the OCP solution using a shorter horizon, making it a practical and effective choice. Compared to the open-loop OCP, reducing the prediction horizon in MPC helps decrease the computational load, making it more suitable for online deployment. In turn, online deployment enables the controller to continuously adapt to changes in the system state, account for disturbances or model-mismatch, and incorporate updated sensor measurements or model corrections, thereby improving robustness and responsiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bb087f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part (b): Linear MPC**\n",
    "\n",
    "\n",
    "In this section, we present the implementation of **model predictive control (MPC)** for linear systems. We begin with a simple case where the system dynamics are assumed to be **linear time-invariant in discrete time** (LTI-DT). This special case is commonly referred to as **linear MPC (LMPC)**.\n",
    "\n",
    "#### **Problem Definition**\n",
    "\n",
    "We consider the following objective for LMPC to compute a sequence of control inputs $ \\{\\boldsymbol{u}_0, \\boldsymbol{u}_1, \\dots, \\boldsymbol{u}_{N-1}\\} $ that minimizes the cost:\n",
    "\n",
    "$$\n",
    "J = g_N(\\boldsymbol{x}_{N}) + \\sum_{k=0}^{N-1} g_k(\\boldsymbol{x}_{k}, \\boldsymbol{u}_k)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "- the linear discrete-time system dynamics\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k, \\quad \\forall k \\in \\{0, \\dots, N-1\\}\n",
    "  $$\n",
    "\n",
    "- state and input constraints:\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{x}_k \\in \\mathcal{X}, \\quad \\forall k \\in \\{0, \\dots, N\\},\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{u}_k \\in \\mathcal{U}, \\quad \\forall k \\in \\{0, \\dots, N-1\\}\n",
    "  $$\n",
    "\n",
    "#### **Cost Design**\n",
    "\n",
    "The cost we used for the following case is designed to be a quadratic cost, which means:\n",
    "\n",
    "- stage cost $g_k(\\boldsymbol{x}_{k}, \\boldsymbol{u}_k)$:\n",
    "\n",
    "$$\n",
    "g_k(\\boldsymbol{x}_{k}, \\boldsymbol{u}_k) =  \\left( \\boldsymbol{x}_k - \\boldsymbol{x}_{\\text{ref}} \\right)^T \\boldsymbol{Q} \\left( \\boldsymbol{x}_k - \\boldsymbol{x}_{\\text{ref}} \\right)\n",
    "+ \\left( \\boldsymbol{u}_k - \\boldsymbol{u}_{\\text{ref}} \\right)^T \\boldsymbol{R} \\left( \\boldsymbol{u}_k - \\boldsymbol{u}_{\\text{ref}} \\right)\n",
    "$$\n",
    "\n",
    "- terminal cost $g_N(\\boldsymbol{x}_{N})$:\n",
    "\n",
    "$$\n",
    "g_N(\\boldsymbol{x}_{N}) = \\left( \\boldsymbol{x}_N - \\boldsymbol{x}_{\\text{ref}} \\right)^T \\boldsymbol{Q}_f \\left( \\boldsymbol{x}_N - \\boldsymbol{x}_{\\text{ref}} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "#### **Lifted-Form Formulation**\n",
    "\n",
    "In the context of MPC, we need to solve a constrained optimization problem over a **finite prediction horizon**. At each time step, we must consider not only the immediate control input, but also how that input will affect future states, costs, and constraints over time.\n",
    "\n",
    "However, if we naively define separate variables for each time step and impose recursive dynamics $ \\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k $ explicitly in our optimization code, we end up with a large number of interdependent variables that are hard to manage, especially when using general-purpose numerical solvers.\n",
    "\n",
    "To systematically express the entire prediction problem as a single standard **quadratic program (QP)**, we introduce the **lifted form**. This formulation involves:\n",
    "\n",
    "- **Stacking all decision variables** (states and controls) into a single vector $ \\boldsymbol{z} $\n",
    "- **Rewriting dynamics, cost, and constraints** in terms of matrix-vector operations on $ \\boldsymbol{z} $\n",
    "\n",
    "which can be rewritten in the following format:\n",
    "\n",
    "$$\n",
    "\\min_z \\quad \\frac{1}{2} \\boldsymbol{z}^T \\boldsymbol{H} \\boldsymbol{z} + \\boldsymbol{f}^T \\boldsymbol{z}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{s.t.} \\quad \\boldsymbol{A}_{\\text{eq}} \\boldsymbol{z} = \\boldsymbol{b}_{\\text{eq}},\n",
    "$$\n",
    "\n",
    "$$\n",
    " \\quad  \\quad  \\boldsymbol{G} \\boldsymbol{z} \\leq \\boldsymbol{h}\n",
    "$$\n",
    "\n",
    "The lifted form offers the following benefits:\n",
    "\n",
    "- ✅ It transforms the MPC problem into a **structured QP** with a standard form:  \n",
    "- ✅ It enables the use of **off-the-shelf QP solvers** (e.g. `cvxopt`, `qpoases`, `osqp`) with minimal modification\n",
    "- ✅ It clarifies the roles of each part of the problem (cost, dynamics, constraints) via matrix structure\n",
    "- ✅ It easily supports extensions, such as soft constraints, time-varying references, and warm-starting\n",
    "\n",
    "##### **a) State and Input Variable in Lifted Form**\n",
    "\n",
    "To formulate the problem as a standard QP, we define a stacked decision variable:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z} :=\n",
    "\\begin{bmatrix}\n",
    "\\boldsymbol{x}_0 \\\\\\\\ \\boldsymbol{x}_1 \\\\\\\\ \\vdots \\\\\\\\ \\boldsymbol{x}_N \\\\\\\\ \\boldsymbol{u}_0 \\\\\\\\ \\boldsymbol{u}_1 \\\\\\\\ \\vdots \\\\\\\\ \\boldsymbol{u}_{N-1}\n",
    "\\end{bmatrix}\n",
    "\\in \\mathbb{R}^{(N+1)n_x + N n_u}\n",
    "$$\n",
    "\n",
    "##### **b) Quadratic Cost in Lifted Form**\n",
    "\n",
    "The cost function is written in standard QP form:\n",
    "\n",
    "$$\n",
    "\\min_z \\quad \\frac{1}{2} \\boldsymbol{z}^T \\boldsymbol{H} \\boldsymbol{z} + \\boldsymbol{f}^T \\boldsymbol{z}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\boldsymbol{H} \\in \\mathbb{R}^{n_z \\times n_z} $ is a block-diagonal matrix:\n",
    "\n",
    "  $$\n",
    "  \\boldsymbol{H} = \\text{diag}(\n",
    "  \\underbrace{\\boldsymbol{Q}, \\dots, \\boldsymbol{Q}}_{N-1 \\text{ times}}, \\boldsymbol{Q}_f, \n",
    "  \\underbrace{\\boldsymbol{R}, \\dots, \\boldsymbol{R}}_{N \\text{ times}}\n",
    "  )\n",
    "  $$\n",
    "\n",
    "- $ f \\in \\mathbb{R}^{n_z} $ contains the linear terms:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f} =\n",
    "\\begin{bmatrix}\n",
    "- \\boldsymbol{Q} \\boldsymbol{x}_{\\text{ref}} \\\\\\\\ \\vdots \\\\\\\\ - \\boldsymbol{Q} \\boldsymbol{x}_{\\text{ref}} \\\\\\\\ - \\boldsymbol{Q}_f \\boldsymbol{x}_{\\text{ref}} \\\\\\\\\n",
    "- \\boldsymbol{R} \\boldsymbol{u}_{\\text{ref}} \\\\\\\\ \\vdots \\\\\\\\ - \\boldsymbol{R} \\boldsymbol{u}_{\\text{ref}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "##### **c) Equality Constraints (Dynamics) in Lifted Form**\n",
    "\n",
    "The system dynamics are encoded into equality constraints:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}_{\\text{eq}} \\boldsymbol{z} = \\boldsymbol{b}_{\\text{eq}}\n",
    "$$\n",
    "\n",
    "Each row of the constraint corresponds to one time step:\n",
    "\n",
    "- For $ k = 0 $:\n",
    "  $$\n",
    "  \\boldsymbol{x}_1 = \\boldsymbol{A} \\boldsymbol{x}_0 + \\boldsymbol{B} \\boldsymbol{u}_0 \\Rightarrow\n",
    "  \\boldsymbol{A}_{\\text{eq}}^{(0)} = [-\\boldsymbol{A} \\; \\boldsymbol{I} \\; \\boldsymbol{0} \\dots \\boldsymbol{0} \\mid -\\boldsymbol{B} \\; \\boldsymbol{0}  \\dots \\boldsymbol{0}]\n",
    "  $$\n",
    "\n",
    "- For $ k = 1, \\dots, N-1 $:\n",
    "  $$\n",
    "  \\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k \\Rightarrow\n",
    "  \\boldsymbol{A}_{\\text{eq}}^{(k)} = [\\boldsymbol{0} \\dots -\\boldsymbol{A} \\; \\boldsymbol{I} \\dots \\boldsymbol{0} \\mid \\boldsymbol{0} \\dots -\\boldsymbol{B} \\dots \\boldsymbol{0}]\n",
    "  $$\n",
    "\n",
    "- If $ \\boldsymbol{x}_0 $ is a variable, we additionally add:\n",
    "  $$\n",
    "  \\boldsymbol{x}_0 = \\boldsymbol{x}_{\\text{init}} \\Rightarrow \\boldsymbol{A}_{\\text{eq}}^{\\text{init}} = [\\boldsymbol{I} \\; \\boldsymbol{0} \\dots \\boldsymbol{0} \\mid \\boldsymbol{0} \\dots \\boldsymbol{0}]\n",
    "  $$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{A}_{\\text{eq}} \\in \\mathbb{R}^{N n_x + n_x \\times n_z}, \\quad \\boldsymbol{b}_{\\text{eq}} = \n",
    "\\begin{bmatrix}\n",
    "\\boldsymbol{x}_{\\text{init}} \\\\\\\\ \\boldsymbol{0} \\\\\\\\ \\vdots \\\\\\\\ \\boldsymbol{0}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "##### **d) Inequality Constraints (State and Input Constraints) in Lifted Form**\n",
    "\n",
    "Input and state bounds are stacked similarly:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{G} \\boldsymbol{z} \\leq \\boldsymbol{h}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ \\boldsymbol{G} $ includes block entries of $ \\pm \\boldsymbol{I} $ applied to state and input components of $ \\boldsymbol{z} $\n",
    "- $ \\boldsymbol{h} $ contains stacked bounds (e.g., $ \\boldsymbol{x}_{\\min}, \\boldsymbol{x}_{\\max}, \\boldsymbol{u}_{\\min}, \\boldsymbol{u}_{\\max} $)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1117bb6a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 1: Linear MPC Implementation**\n",
    "\n",
    "In this step, we implement the core logic of a linear MPC based on a standard **QP** formulation using the **lifted form** and solved via the `cvxopt` solver. This implementation assumes that the system dynamics are linear time-invariant (LTI), and it expresses the full finite-horizon optimal control problem as a single constrained quadratic program.\n",
    "\n",
    "The implementation follows the classical lifted-form pipeline and includes four key components:\n",
    "\n",
    "1\\) **Linearize the system** and compute the discrete-time matrices $ \\boldsymbol{A}_d $, $ \\boldsymbol{B}_d $ at the current state;  \n",
    "   *Hint: use the `get_linearized_AB_discrete()` method from the `Dynamics` class with current state and a nominal control input (typically zero).*\n",
    "\n",
    "2\\) **Construct the cost function** in QP form:  \n",
    "\n",
    "   The QP cost is written as:  \n",
    "\n",
    "   $$\n",
    "   J = \\frac{1}{2} \\boldsymbol{z}^\\top \\boldsymbol{H} \\boldsymbol{z} + \\boldsymbol{f}^\\top \\boldsymbol{z}\n",
    "   $$\n",
    "\n",
    "   where $ \\boldsymbol{z} $ is the stacked vector of states and inputs over the horizon:  \n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{z} = [\\boldsymbol{x}_0^\\top, \\boldsymbol{x}_1^\\top, \\dots, \\boldsymbol{x}_N^\\top, \\boldsymbol{u}_0^\\top, \\dots, \\boldsymbol{u}_{N-1}^\\top]^\\top\n",
    "   $$\n",
    "\n",
    "   - Matrix `H` is block-diagonal with weights $ \\boldsymbol{Q}, \\boldsymbol{Q}_f, \\boldsymbol{R} $ repeated across the horizon;\n",
    "\n",
    "   - Vector `f` encodes target tracking;\n",
    "\n",
    "3\\) **Formulate the dynamics as equality constraints**:  \n",
    "\n",
    "   The dynamics constraints are stacked in lifted form:\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{A}_{\\text{eq}} \\boldsymbol{z} = \\boldsymbol{b}_{\\text{eq}}\n",
    "   $$\n",
    "\n",
    "   - Each row encodes $ \\boldsymbol{x}_{k+1} = \\boldsymbol{A} \\boldsymbol{x}_k + \\boldsymbol{B} \\boldsymbol{u}_k $\n",
    "\n",
    "   - The first row optionally enforces $ \\boldsymbol{x}_0 = \\boldsymbol{x}_{\\text{init}} $ as a hard constraint\n",
    "\n",
    "4\\) **Define inequality constraints** for states and controls:  \n",
    "\n",
    "   These are encoded as:\n",
    "\n",
    "   $$\n",
    "   \\boldsymbol{G} \\boldsymbol{z} \\leq \\boldsymbol{h}\n",
    "   $$\n",
    "\n",
    "   - Each constraint uses $\\pm \\boldsymbol{I}$ applied to specific blocks of the decision vector `z`\n",
    "\n",
    "   - You can access lower/upper bounds from `env.state_lbs`, `env.input_ubs`, etc.\n",
    "\n",
    "5\\) **Solve the QP** using the `cvxopt` interface:  \n",
    "\n",
    "   ```python\n",
    "                                sol = solvers.qp(matrix(H), matrix(f), matrix(G), matrix(h), matrix(Aeq), matrix(beq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506c36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cvxopt import matrix, solvers\n",
    "\n",
    "class LinearMPCController:\n",
    "    def __init__(self, \n",
    "            env: Env, \n",
    "            dynamics: Dynamics, \n",
    "            Q: np.ndarray, \n",
    "            R: np.ndarray, \n",
    "            Qf: np.ndarray, \n",
    "            freq: float, \n",
    "            N: int, \n",
    "            name: str = 'LMPC', \n",
    "            type: str = 'MPC', \n",
    "            verbose: bool = False\n",
    "        ) -> None:\n",
    "\n",
    "        self.env = env\n",
    "        self.dynamics = dynamics\n",
    "\n",
    "        self.Q = Q\n",
    "        self.R = R\n",
    "        self.Qf = Qf\n",
    "\n",
    "        self.N = N\n",
    "\n",
    "        self.name = name\n",
    "        self.type = type\n",
    "\n",
    "        self.freq = freq\n",
    "        self.dt = 1.0 / freq\n",
    "        \n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.dim_states = dynamics.dim_states\n",
    "        self.dim_inputs = dynamics.dim_inputs\n",
    "\n",
    "        self.x_pred = None\n",
    "        self.u_pred = None\n",
    "    \n",
    "    @check_input_constraints\n",
    "    def compute_action(self, current_state, current_time=None):\n",
    "\n",
    "        x0 = current_state.reshape(-1, 1)\n",
    "        u0 = np.zeros((self.dim_inputs, 1))\n",
    "\n",
    "        A_d, B_d = self.dynamics.get_linearized_AB_discrete(x0, u0, self.dt)\n",
    "\n",
    "        n_vars = self.dim_states * (self.N + 1) + self.dim_inputs * self.N\n",
    "\n",
    "        # Cost\n",
    "        H = np.zeros((n_vars, n_vars))\n",
    "        f = np.zeros((n_vars, 1))\n",
    "\n",
    "        x_ref = self.env.target_state.reshape(-1, 1)  # assumed constant\n",
    "        u_ref = np.array(self.dynamics.get_equilibrium_input(x_ref)).reshape(-1, 1)        # assumed constant\n",
    "\n",
    "        for k in range(self.N):\n",
    "            idx_x = slice(k * self.dim_states, (k + 1) * self.dim_states)\n",
    "            idx_u = slice((self.N + 1) * self.dim_states + k * self.dim_inputs,\n",
    "                          (self.N + 1) * self.dim_states + (k + 1) * self.dim_inputs)\n",
    "\n",
    "            Qk = self.Q\n",
    "            H[idx_x, idx_x] = Qk\n",
    "            H[idx_u, idx_u] = self.R\n",
    "\n",
    "            f[idx_x] = -Qk @ x_ref\n",
    "            f[idx_u] = -self.R @ u_ref\n",
    "\n",
    "        # Terminal cost\n",
    "        idx_x_terminal = slice(self.N * self.dim_states, (self.N + 1) * self.dim_states)\n",
    "        H[idx_x_terminal, idx_x_terminal] = self.Qf\n",
    "        f[idx_x_terminal] = -self.Qf @ x_ref\n",
    "\n",
    "        # Dynamics constraints\n",
    "        Aeq = []\n",
    "        beq = []\n",
    "\n",
    "        for k in range(self.N):\n",
    "            row = np.zeros((self.dim_states, n_vars))\n",
    "            idx_xk = slice(k * self.dim_states, (k + 1) * self.dim_states)\n",
    "            idx_xk_next = slice((k + 1) * self.dim_states, (k + 2) * self.dim_states)\n",
    "            idx_uk = slice((self.N + 1) * self.dim_states + k * self.dim_inputs,\n",
    "                           (self.N + 1) * self.dim_states + (k + 1) * self.dim_inputs)\n",
    "            row[:, idx_xk_next] = -np.eye(self.dim_states)\n",
    "            row[:, idx_xk] = A_d\n",
    "            row[:, idx_uk] = B_d\n",
    "            Aeq.append(row)\n",
    "            beq.append(np.zeros((self.dim_states, 1)))\n",
    "\n",
    "        # Add x0 = current_state as hard equality constraint\n",
    "        row0 = np.zeros((self.dim_states, n_vars))\n",
    "        row0[:, :self.dim_states] = np.eye(self.dim_states)\n",
    "        Aeq.insert(0, row0)\n",
    "        beq.insert(0, x0)\n",
    "\n",
    "        Aeq = np.vstack(Aeq)\n",
    "        beq = np.vstack(beq)\n",
    "\n",
    "        # Inequality constraints\n",
    "        G_list = []\n",
    "        h_list = []\n",
    "\n",
    "        for k in range(self.N + 1):\n",
    "            # State constraints\n",
    "            if self.env.state_lbs is not None:\n",
    "                Gx_l = np.zeros((self.dim_states, n_vars))\n",
    "                Gx_l[:, k * self.dim_states:(k + 1) * self.dim_states] = -np.eye(self.dim_states)\n",
    "                G_list.append(Gx_l)\n",
    "                h_list.append(-np.array(self.env.state_lbs).reshape(-1, 1))\n",
    "            if self.env.state_ubs is not None:\n",
    "                Gx_u = np.zeros((self.dim_states, n_vars))\n",
    "                Gx_u[:, k * self.dim_states:(k + 1) * self.dim_states] = np.eye(self.dim_states)\n",
    "                G_list.append(Gx_u)\n",
    "                h_list.append(np.array(self.env.state_ubs).reshape(-1, 1))\n",
    "\n",
    "        for k in range(self.N):\n",
    "            # Input constraints\n",
    "            if self.env.input_lbs is not None:\n",
    "                Gu_l = np.zeros((self.dim_inputs, n_vars))\n",
    "                Gu_l[:, (self.N + 1) * self.dim_states + k * self.dim_inputs:\n",
    "                          (self.N + 1) * self.dim_states + (k + 1) * self.dim_inputs] = -np.eye(self.dim_inputs)\n",
    "                G_list.append(Gu_l)\n",
    "                h_list.append(-np.array(self.env.input_lbs).reshape(-1, 1))\n",
    "            if self.env.input_ubs is not None:\n",
    "                Gu_u = np.zeros((self.dim_inputs, n_vars))\n",
    "                Gu_u[:, (self.N + 1) * self.dim_states + k * self.dim_inputs:\n",
    "                          (self.N + 1) * self.dim_states + (k + 1) * self.dim_inputs] = np.eye(self.dim_inputs)\n",
    "                G_list.append(Gu_u)\n",
    "                h_list.append(np.array(self.env.input_ubs).reshape(-1, 1))\n",
    "\n",
    "        # Check if there are any constraints\n",
    "        if len(G_list) > 0:\n",
    "            G = np.vstack(G_list)\n",
    "            h = np.vstack(h_list)\n",
    "        else:\n",
    "            G = np.zeros((1, n_vars))\n",
    "            h = np.array([[1e10]])\n",
    "\n",
    "        # Solve the QP\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(matrix(H), matrix(f), matrix(G), matrix(h), matrix(Aeq), matrix(beq))\n",
    "        z_opt = np.array(sol['x']).flatten()\n",
    "\n",
    "        x_opt = z_opt[: (self.N + 1) * self.dim_states].reshape(self.N + 1, self.dim_states).T\n",
    "        u_opt = z_opt[(self.N + 1) * self.dim_states:].reshape(self.N, self.dim_inputs).T\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"Optimal control action: {u_opt[:, 0]}\")\n",
    "            print(f\"Predicted x: {x_opt}\")\n",
    "            print(f\"Predicted u: {u_opt}\")\n",
    "\n",
    "        self.x_pred = x_opt\n",
    "        self.u_pred = u_opt\n",
    "\n",
    "        return u_opt[:, 0].flatten()+u_ref, x_opt.T, u_opt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcef2ba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 2: Simulation and Visualization**  \n",
    "\n",
    "1\\) Specify the arguments and instantiate the controller class `LinearMPCController` as `LMPC`; \n",
    "\n",
    "- Parameters in the controller design:  \n",
    "\n",
    "    i) weight for state $\\bm{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\bm{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "    iv) horizon $N = 20$\n",
    "\n",
    "2\\) Instantiate the class `Simulator` and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "3\\) Instantiate the class `Visualizer`, call function `display_final_results()` and `display_animation()` to show the simulations;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d4c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "N = 20\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 8 # time length of simulation\n",
    "\n",
    "# Define the LMPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, name='LMPC')\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_plots()\n",
    "visualizer_mpc.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c4e48",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 3: Horizon and Terminal Components**  \n",
    "\n",
    "In this section, we explore the effect of MPC hyperparameters on the overall control performance. Refer to the following instructions to run the simulations and analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc42e7b",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **🔍 Hands-on Exploration: influence of the prediction horizon $N$ (primary)**\n",
    "\n",
    "To gain an initial understanding of how the prediction horizon $N$ affects the behavior of the MPC controller, vary $N$ (ranging from $20$ to $160$) and repeat the simulations to observe the results.\n",
    "\n",
    "Key aspects to pay attention to include:\n",
    "\n",
    "- How similar are the MPC policy and the OCP policy?\n",
    "\n",
    "- How does the computation time compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1673ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction horizon in MPC\n",
    "N = 20 # you may try different values from 20 to 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c36209",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weight matrix in terrmianl cost: {Qf}\")\n",
    "\n",
    "# Define the LMPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, name='MPC') \n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "print(f\"Average computation time per step: {(time.time()-start_time) / (freq * t_terminal)}\")\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68012818",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer_mpc.display_contrast_plots(simulator_ocp_2n)\n",
    "visualizer_mpc.display_contrast_animation_same(simulator_ocp_2n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff92bc",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "By comparing the simulation results of full-horizon open-loop OCP and MPC, we can draw the following conclusions:\n",
    "\n",
    "- When the prediction horizon $N$ is relatively small:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\oplus$ **The computation time required for each MPC update is significantly shorter than that of solving the full OCP**. For example, when $N=20$, the average computation time is only $4.5$ ms, which makes online deployment much more feasible. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\ominus$ However, at the same time, the resulting **MPC policy is noticeably less aggressive compared to the full-horizon OCP solution**. This is because the MPC is \"short-sighted\" and only finds an optimal solution over the reduced horizon.\n",
    "\n",
    "- As the prediction horizon $N$ increases, the MPC policy progressively approaches the full-horizozn OCP policy. When $N=160$, the two policies are nearly identical. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0607db",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**While a longer prediction horizon enables the controller to plan more aggressive and optimal trajectories, it also introduces a substantial computational burden.** This trade-off between control performance and real-time feasibility is a central consideration in practical MPC design.\n",
    "\n",
    "Fortunately, this issue can be mitigated by introducing well-designed terminal components, such as a terminal cost. An appropriate terminal cost function accounts for the short-sightedness of the MPC and captures the impact of any future steps. For LMPC with a quadratic cost, a common approach is to **use the total cost of an infinite-horizon LQR as the terminal cost**.\n",
    "\n",
    "Please run the example below and observe how this setting influences the closed-loop performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddefb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction horizon in MPC\n",
    "N = 20 # set the horizon to be a small value\n",
    "\n",
    "# Linearize dynamics at equilibrium and solve DARE to compute the infinite LQR weight matrix\n",
    "A_d, B_d = dynamics.get_linearized_AB_discrete(np.array([0.0, 0.0]), 0.0, 1/freq)\n",
    "P = scipy.linalg.solve_discrete_are(A_d, B_d, Q, R)\n",
    "\n",
    "print(f\"Weight matrix in terrmianl cost: {P}\")\n",
    "\n",
    "# Define the LMPC controller\n",
    "controller_mpc = LinearMPCController(env_constr, dynamics_constr, Q, R, P, freq, N, name='MPC') \n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_contrast_plots(simulator_ocp_2n)\n",
    "visualizer_mpc.display_contrast_animation_same(simulator_ocp_2n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0c3c8",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "By incorporating the infinite-horizon LQR cost-to-go as the terminal cost in LMPC, the controller can achieve an aggressive and near-optimal policy, even with a relatively short prediction horizon $N$. **This demonstrates that a well-designed terminal cost can effectively compensate for a limited horizon, enabling a balanced trade-off between online computational tractability and policy optimality.**\n",
    "\n",
    "The **key insight** behind this phenomenon lies in the structure of optimal control problems. When the prediction horizon is short, the controller does not plan for long-term consequences, often leading to suboptimal control inputs.\n",
    "However, by embedding the LQR cost-to-go as the terminal cost, we provide the controller with an approximation of the infinite-horizon value function beyond the finite horizon.\n",
    "This effectively extends the controller’s planning capability without increasing the computational burden, allowing it to make decisions that account for long-term benefits while operating within a short horizon.\n",
    "In essence, the terminal cost serves as a surrogate for future cost.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab289ea",
   "metadata": {},
   "source": [
    "#### **Task 4: LMPC vs. LQR**  \n",
    "\n",
    "In this section, we compare linear MPC and LQR from Chapter 2. For LQR, explicit constraint handling is not supported. Therefore, we mimic the \"clipping\" behavior observed in physical systems — if the computed control input exceeds the allowed bounds, it is truncated to stay within the specified limits. In contrast, the MPC controller supports direct incorporation of constraints, allowing us to explicitly define input and state bounds during optimization.\n",
    "\n",
    "More detailed, the case index and the constraints are:\n",
    "\n",
    "- Test case: case 1 (flat ground)\n",
    "\n",
    "- State constraints: $ \\quad p \\in [-2.0, 2.0], \\quad v \\in [-1.0, 1.0],$  \n",
    "\n",
    "- Input constraints: $ \\quad a \\in [-0.25, 0.25],$  \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*Note that: in this example we deliberately use a tight input constraint to highlight the impact of trajectory saturation on the controller’s behavior.*\n",
    "\n",
    "By introducing these constraints, we simulate the system's response and contrast the two controllers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04352f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear case number\n",
    "case_linear = 1\n",
    "\n",
    "# Define the physical boundary condition\n",
    "input_lbs = -0.25\n",
    "input_ubs = 0.25\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env'\n",
    "env_linear = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_linear = Dynamics(env_linear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_linear = LQRController(env_linear, dynamics_linear, Q, R, freq, name='LQR_case1_ideal')\n",
    "controller_lqr_linear.set_lin_point(np.array([0.0, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear = Simulator(dynamics_linear, controller_lqr_linear, env, 1/freq, t_terminal)\n",
    "simulator_lqr_linear.run_simulation()\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_linear_clipped = Env(case_linear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]), \n",
    "                           input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_linear_clipped = LQRController(env_linear_clipped, dynamics_linear, Q, R, freq, name='LQR_case1_clipped')\n",
    "controller_lqr_linear_clipped.set_lin_point(np.array([0.0, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_linear_clipped = Simulator(dynamics_linear, controller_lqr_linear_clipped, env_linear_clipped, 1/freq, t_terminal)\n",
    "simulator_lqr_linear_clipped.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lqr_linear_clipped = Visualizer(simulator_lqr_linear_clipped)\n",
    "visualizer_lqr_linear_clipped.display_contrast_plots(simulator_lqr_linear)\n",
    "visualizer_lqr_linear_clipped.display_contrast_animation(simulator_lqr_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19f1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_linear_constr = MPCController(env_linear_clipped, dynamics_linear, Q, R, Qf, freq, N, name=\"MPC_case1_constr\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_linear_constr = Simulator(dynamics_linear, controller_mpc_linear_constr, env_linear_clipped, 1/freq, t_terminal)\n",
    "simulator_mpc_linear_constr.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_linear_constr = Visualizer(simulator_mpc_linear_constr)\n",
    "visualizer_mpc_linear_constr.display_plots()\n",
    "visualizer_mpc_linear_constr.display_animation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02789675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show comparison between 2 cases\n",
    "visualizer_mpc_linear_constr.display_contrast_plots(simulator_lqr_linear_clipped)\n",
    "visualizer_mpc_linear_constr.display_contrast_animation_same(simulator_lqr_linear_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a385974",
   "metadata": {},
   "source": [
    "#### **Results Analysis:**\n",
    "\n",
    "Simulation results indicate that LQR, due to its inability to explicitly enforce input and state constraints, may generate control policies that exceed these bounds during execution. In such cases, actuator-level clipping leads to saturation, causing the actual control trajectory to deviate from the ideal one. Nevertheless, thanks to the global convergence property of linear systems, the controller still manages to reach the target state, albeit with noticeable delays.\n",
    "\n",
    "In contrast, LMPC is able to explicitly handle both input and state constraints within the optimization process, thereby avoiding such trajectory saturations and ensuring constraint-compliant tracking performance.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c0d4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part (c): Nonlinear MPC**\n",
    "\n",
    "In this section, we will provide the implementation of nonlinear MPC (NMPC) and demonstrate how it can be applied to a stabilization task. We will also visualize the performance of the implemented NMPC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b54765f",
   "metadata": {},
   "source": [
    "##### **NMPC Problem Formulation:**\n",
    "\n",
    "$$\n",
    "J_k^*(\\boldsymbol{x_k}) = \\min_{u_{k|k}, \\ldots, u_{k+N-1|k}} \n",
    "g_N\\left( \\boldsymbol{x_{k+N|k}} \\right) + \\sum_{i=0}^{N-1} g_i\\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right)\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i+1|k}} = \\boldsymbol{f} \\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right), \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+i|k}} \\in \\mathcal{X}, \\quad \\forall i \\in \\{0, \\ldots, N\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_{k+i|k} \\in \\mathcal{U}, \\quad \\forall i \\in \\{0, \\ldots, N-1\\},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+N|k}} \\in \\mathcal{X}_f, \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k|k}} = \\boldsymbol{\\overline{x}_{k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee16a22",
   "metadata": {},
   "source": [
    "##### **Cost design in stabilization task:**\n",
    "\n",
    "- Stage cost: The cost function consists of a tracking error term and an input penalty term. The tracking error term minimizes the deviation between the state trajectory and the target state, while the input penalty term prevents the controller from applying excessively large.\n",
    "\n",
    "$$\n",
    "g_i\\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right) = \\left( \\boldsymbol{x_{k+i|k}} - \\boldsymbol{x_T} \\right)^T \\boldsymbol{Q} \\left( \\boldsymbol{x_{k+i|k}} - \\boldsymbol{x_T} \\right) + u_{k+i|k} ^T \\boldsymbol{R} u_{k+i|k}\n",
    "$$\n",
    "\n",
    "- Terminal cost: only the tracking error term\n",
    "\n",
    "$$\n",
    "g_N\\left( \\boldsymbol{x_{k+N|k}} \\right) = \\left( \\boldsymbol{x_{k+N|k}} - \\boldsymbol{x_T} \\right)^T \\boldsymbol{Q_f} \\left( \\boldsymbol{x_{k+N|k}} - \\boldsymbol{x_T} \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7580d",
   "metadata": {},
   "source": [
    "##### **Reformulation of the Original NLP Problem:**\n",
    "\n",
    "For nonlinear programming (NLP) problems that employ a quadratic cost function, a common approach is to reformulate the problem as a quadratic cost optimization problem with nonlinear constraints to leverage specialized solvers. In this reformulation, the state and control variables at each shooting node are combined into a single augmented variable, and the cost function is rewritten as a quadratic function of this new augmented variable. The reformulated states and costs can be expressed as follows:\n",
    "\n",
    "- Intermediate shooting node (related to stage cost term $\\forall i \\in \\{0, \\ldots, N-1\\}$): \n",
    "\n",
    "$$\n",
    "\\boldsymbol{z_{k+i|k}} = [\\boldsymbol{x_{k+i|k}}^T, \\boldsymbol{u_{k+i|k}}^T]^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z}_{ref} = [\\boldsymbol{x_T}^T, \\boldsymbol{0}^{1 \\times m}]^T,\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_i\\left( \\boldsymbol{x_{k+i|k}}, \\boldsymbol{u_{k+i|k}} \\right) = \\left( \\boldsymbol{z_{k+i|k}} - \\boldsymbol{z}_{ref} \\right)^T \\begin{bmatrix} \\boldsymbol{Q} & \\boldsymbol{0}^{n \\times m} \\\\ \\boldsymbol{0}^{m \\times n} & \\boldsymbol{R} \\end{bmatrix} \\left( \\boldsymbol{z_{k+i|k}} - \\boldsymbol{z}_{ref} \\right)\n",
    "$$\n",
    "\n",
    "- Terminal shooting node (related to terminal cost term $i = N$):\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z_{k+N|k}} = \\boldsymbol{x_{k+N|k}},\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{z}_{f,ref} = \\boldsymbol{x_T}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "g_N\\left( \\boldsymbol{x_{k+N|k}} \\right) = \\left( \\boldsymbol{z_{k+N|k}} - \\boldsymbol{z}_{f,ref} \\right)^T \\boldsymbol{Q_f} \\left( \\boldsymbol{z_{k+N|k}} - \\boldsymbol{z}_{f,ref} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a86e0",
   "metadata": {},
   "source": [
    "##### **Solving the NMPC Problem:**\n",
    "\n",
    "In the linear MPC section, we showed how the optimal control problem (OCP) can be reformulated into a lifted form as a QP, where all decision variables are stacked together, and the dynamics are expressed as linear equality constraints.\n",
    "\n",
    "However, in nonlinear MPC, the system dynamics are nonlinear:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{x_{k+1}} = \\boldsymbol{f} \\left( \\boldsymbol{x_{k}}, \\boldsymbol{u_{k}} \\right)\n",
    "$$\n",
    "\n",
    "This leads to a nonlinear programming (NLP) problem once we stack all decision variables. To solve such problems, we need to resort to **Sequential Quadratic Programming (SQP)**, which we introduced in the **Optimization Fundamentals** chapter. SQP iteratively approximates the nonlinear problem using a sequence of QP subproblems that are solved at each step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ded12e2",
   "metadata": {},
   "source": [
    "##### **NMPC Solver:**\n",
    "\n",
    "Rewriting and solving nonlinear OCPs manually is often **cumbersome and error-prone**. To streamline implementation, we can use specialized optimal control toolkits that:\n",
    "\n",
    "- Automatically handle dynamics, cost, and constraint modeling \n",
    "\n",
    "- Generate efficient code for fast real-time solutiond;\n",
    "\n",
    "Some widely used NMPC interfaces and solvers include:\n",
    "\n",
    "- **Acados (Interface)**: fast, tailored for OCP problems, supports real-time code generation;\n",
    "\n",
    "- **CasADi (Interface) + IPOPT (Solver)**: flexible, symbolic modeling;\n",
    "\n",
    "- Solvers: **FORCES Pro**, **HPIPM**, and others;\n",
    "\n",
    "In this section, we use **Acados** for our implementation since it is tailored to optimal control problems and is, therefore, particularly computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d49cc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 1: Solver Configuration**  \n",
    "\n",
    "In this step, we will implement the setup function `setup_external()`, which will be automatically called in the constructor of class `MPCController`. We use Acados as the solver for our MPC problem, which offers a rich Python interface for defining and solving optimization problems. The Python interface of Acados allows users to describe the dynamics, cost, and constraints symbolically. It then handles code generation, compilation, and solver setup internally, enabling fast and efficient online optimization. The implementation will follow three key steps:\n",
    "\n",
    "1\\) Instantiate the class `AcadosModel`, specify the state / input vector and the symbolic system dynamcis;  \n",
    "    Hint: you may get the state / input vectors and the system dynamcis from class `Dynamics` \n",
    "\n",
    "2\\) Instantiate the class `AcadosOcp`, specify the model and the optimization problem (incl. solver configurations, costs, constraints, etc.);  \n",
    "    Hint 1: for the standard formulation of an optimal control problem in Acados please refer to the attachment [problem_formulation_ocp_acados.pdf](./problem_formulation_ocp_acados.pdf)    \n",
    "    Hint 2: the python interfaces provided by Acados for solver configuration please refer to this [website](https://docs.acados.org/python_interface/index.html)  \n",
    "\n",
    "\n",
    "3\\) Instantiate the class `AcadosOcpSolver` with the object of class `AcadosOcp`; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a475876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_external(self) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Define the MPC optimization problem using Acados.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Model\n",
    "    # Set up Acados model\n",
    "    model = AcadosModel()\n",
    "    model.name = self.name\n",
    "\n",
    "    # Define model: x_dot = f(x, u)\n",
    "    model.x = self.dynamics.states\n",
    "    model.u = self.dynamics.inputs\n",
    "    model.f_expl_expr = ca.vertcat(self.dynamics.dynamics_function(self.dynamics.states, self.dynamics.inputs))\n",
    "    model.f_impl_expr = None # no needed, we already have the explicit model\n",
    "\n",
    "\n",
    "    ## Optimal control problem\n",
    "    # Set up Acados OCP\n",
    "    ocp = AcadosOcp()\n",
    "    ocp.model = model # link to the model (class: AcadosModel)\n",
    "    ocp.solver_options.N_horizon = self.N\n",
    "    ocp.solver_options.tf = self.N * self.dt  # total prediction time\n",
    "    ocp.solver_options.qp_solver = \"FULL_CONDENSING_HPIPM\" # Partially condensing interior-point method\n",
    "    ocp.solver_options.integrator_type = \"ERK\" # explicit Runge-Kutta\n",
    "    ocp.solver_options.nlp_solver_type = \"SQP\" # sequential quadratic programming\n",
    "\n",
    "    # Set up other hyperparameters in SQP solving\n",
    "    ocp.solver_options.nlp_solver_max_iter = 100\n",
    "    ocp.solver_options.nlp_solver_tol_stat = 1E-6\n",
    "    ocp.solver_options.nlp_solver_tol_eq = 1E-6\n",
    "    ocp.solver_options.nlp_solver_tol_ineq = 1E-6\n",
    "    ocp.solver_options.nlp_solver_tol_comp = 1E-6\n",
    "    \n",
    "    # For debugging\n",
    "    #ocp.solver_options.print_level = 2\n",
    "\n",
    "    # Set up cost function\n",
    "    ocp.cost.cost_type = \"LINEAR_LS\"\n",
    "    ocp.cost.cost_type_e = \"LINEAR_LS\"\n",
    "    ocp.cost.W = np.block([\n",
    "        [self.Q, np.zeros((self.dim_states, self.dim_inputs))],\n",
    "        [np.zeros((self.dim_inputs, self.dim_states)), self.R],\n",
    "    ])\n",
    "    ocp.cost.W_e = self.Qf\n",
    "\n",
    "    # Set up mapping from QP to OCP\n",
    "    # Define output matrix for non-terminal state\n",
    "    ocp.cost.Vx = np.block([\n",
    "        [np.eye(self.dim_states)],\n",
    "        [np.zeros((self.dim_inputs, self.dim_states))]\n",
    "    ])\n",
    "    # Define breakthrough matrix for non-terminal state\n",
    "    ocp.cost.Vu = np.block([\n",
    "        [np.zeros((self.dim_states, self.dim_inputs))],\n",
    "        [np.eye(self.dim_inputs)]\n",
    "    ])\n",
    "    # Define output matrix for terminal state\n",
    "    ocp.cost.Vx_e = np.eye(self.dim_states)\n",
    "\n",
    "    # Initialize reference of task (stabilization)\n",
    "    ocp.cost.yref = np.zeros(self.dim_states + self.dim_inputs) \n",
    "    ocp.cost.yref_e = np.zeros(self.dim_states) \n",
    "\n",
    "    # Define constraints\n",
    "    ocp.constraints.x0 = self.init_state  # Initial state\n",
    "\n",
    "    # State constraints\n",
    "    ocp.constraints.idxbx = np.arange(self.dim_states)\n",
    "    ocp.constraints.idxbx_e = np.arange(self.dim_states)\n",
    "    if self.env.state_lbs is None and self.env.state_ubs is None:\n",
    "        ocp.constraints.lbx_0 = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx_0 = np.full(self.dim_states, 1e6)\n",
    "        ocp.constraints.lbx = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx = np.full(self.dim_states, 1e6)\n",
    "        ocp.constraints.lbx_e = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx_e = np.full(self.dim_states, 1e6)\n",
    "    elif self.env.state_lbs is not None and self.env.state_ubs is None:\n",
    "        ocp.constraints.lbx_0 = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_0 = np.full(self.dim_states, 1e6)\n",
    "        ocp.constraints.lbx = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx = np.full(self.dim_states, 1e6)\n",
    "        ocp.constraints.lbx_e = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_e = np.full(self.dim_states, 1e6)\n",
    "    elif self.env.state_lbs is None and self.env.state_ubs is not None:\n",
    "        ocp.constraints.lbx_0 = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx_0 = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx_e = np.full(self.dim_states, -1e6)\n",
    "        ocp.constraints.ubx_e = np.array(self.env.state_ubs)\n",
    "    else:\n",
    "        ocp.constraints.lbx_0 = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_0 = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx_e = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_e = np.array(self.env.state_ubs)\n",
    "    \n",
    "    # Input constraints\n",
    "    ocp.constraints.idxbu = np.arange(self.dim_inputs)\n",
    "    if self.env.input_lbs is None and self.env.input_ubs is None:\n",
    "        ocp.constraints.lbu = np.full(self.dim_inputs, -1e6)\n",
    "        ocp.constraints.ubu = np.full(self.dim_inputs, 1e6)\n",
    "    elif self.env.input_lbs is not None and self.env.input_ubs is None:\n",
    "        ocp.constraints.lbu = np.array(self.env.input_lbs)\n",
    "        ocp.constraints.ubu = np.full(self.dim_inputs, 1e6)\n",
    "    elif self.env.input_lbs is None and self.env.input_ubs is not None:\n",
    "        ocp.constraints.lbu = np.full(self.dim_inputs, -1e6)\n",
    "        ocp.constraints.ubu = np.array(self.env.input_ubs)\n",
    "    else:\n",
    "        ocp.constraints.lbu = np.array(self.env.input_lbs)\n",
    "        ocp.constraints.ubu = np.array(self.env.input_ubs)\n",
    "\n",
    "\n",
    "    ## Ocp Solver\n",
    "    # Set up Acados solver\n",
    "    self.ocp = ocp\n",
    "    self.solver = AcadosOcpSolver(ocp, json_file=f\"{model.name}.json\")\n",
    "\n",
    "    if self.verbose:\n",
    "        print(\"MPC setup with Acados completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9973ad",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 2: Control Loop Definition**  \n",
    "\n",
    "After defining the optimization problem using Acados' Python interface and generating the corresponding solver package, we need to assign values to variables and compute the control output in each control cycle. This process will be encapsulated in a control loop function `compute_action_external()`, which is called at every time step. In each cycle, you need to:\n",
    "\n",
    "1\\) Assign values for the initial state and the tracking reference via the interface `set()`;  \n",
    "\n",
    "2\\) Solve OCP problem by calling `solve()`;  \n",
    "\n",
    "3\\) Get the first input command through `get()`;  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c028e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_external(self, current_state: np.ndarray, current_time) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Solve the MPC problem and compute the optimal control action.\n",
    "\n",
    "    Args:\n",
    "    - current_state: The current state of the system.\n",
    "    - current_time: The current time (not used in this time-invariant case).\n",
    "\n",
    "    Returns:\n",
    "    - Optimal control action.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Current state: {current_state}\")\n",
    "    print(f\"Target state: {self.target_state}\")\n",
    "\n",
    "\n",
    "    # Update initial state in the solver\n",
    "    self.solver.set(0, \"lbx\", current_state)\n",
    "    self.solver.set(0, \"ubx\", current_state)\n",
    "\n",
    "    # Update reference trajectory for all prediction steps\n",
    "    state_ref = self.target_state\n",
    "    input_ref = np.zeros(self.dim_inputs)\n",
    "    for i in range(self.N):\n",
    "        self.solver.set(i, \"yref\", np.concatenate((state_ref, input_ref)))\n",
    "    self.solver.set(self.N, \"yref\", state_ref) # set reference valur for y_N seperately (different shape)\n",
    "\n",
    "    # Solve the MPC problem\n",
    "    status = self.solver.solve()\n",
    "    #if status != 0:\n",
    "    #    raise ValueError(f\"Acados solver failed with status {status}\")\n",
    "\n",
    "    # Extract the first control action\n",
    "    u_optimal = self.solver.get(0, \"u\")\n",
    "\n",
    "    # Extract the predictions\n",
    "    x_pred = np.zeros((self.N + 1, self.dim_states))\n",
    "    u_pred = np.zeros((self.N, self.dim_inputs))\n",
    "    for i in range(self.N + 1):\n",
    "        x_pred[i, :] = self.solver.get(i, \"x\")\n",
    "        if i < self.N:\n",
    "            u_pred[i, :] = self.solver.get(i, \"u\")\n",
    "\n",
    "    if self.verbose:\n",
    "        print(f\"Optimal control action: {u_optimal}\")\n",
    "        #print(f\"x_pred: {x_pred}\")\n",
    "        #print(f\"u_pred: {u_pred}\")\n",
    "\n",
    "    return u_optimal, x_pred, u_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e6160",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 3: Simulation and Visualization**  \n",
    "\n",
    "1\\) Bind the defined setup function `setup_external()` and control loop function `compute_action_external()` to class `MPCController`;\n",
    "\n",
    "2\\) Specify the arguments and instantiate the controller class `MPCController` as `MPC_0`; \n",
    "\n",
    "- Parameters in the controller design:  \n",
    "\n",
    "    i) weight for state $\\bm{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "    ii) weight for input $\\bm{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "    iii) control frequency $f = 20$\n",
    "\n",
    "    iv) horizon $N = 20$\n",
    "\n",
    "3\\) Instantiate the class `Simulator` and call function `run_simulation()` to generate the simulated state- and input-trajectory;\n",
    "\n",
    "4\\) Instantiate the class `Visualizer`, call function `display_final_results()` and `display_animation()` to show the simulations;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f87f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bind the defined MPC controller setup function to the corresponding class, will be automatically called by constructor\n",
    "MPCController.setup = setup_external\n",
    "MPCController.compute_action = compute_action_external\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "N = 20\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 8 # time length of simulation\n",
    "\n",
    "# Instantiate the MPC controller class\n",
    "# Arguments: \n",
    "#   1) an object of class `Env` (to deliver infos about initial state, constraints, etc.), type: Env  \n",
    "#   2) an object of class `Dynamics` (to deliver infos about symbolic system dynamics), type: Dynamics  \n",
    "#   3) weight matrices in cost functions: i) `Q`: weight metrix of state, type: np.array  \n",
    "#                                         ii) `R`: weight metrix of input, type: np.array  \n",
    "#                                         iii) `Q_f`: weight metrix of state in terminal cost, type: np.array  \n",
    "#   4) freq: control frequency $f$ , type: int  \n",
    "#   5) N: horizon in MPC , type: int  \n",
    "#   6) name: the name of current controller displayed in plots, type: string\n",
    "controller_mpc = MPCController(env_constr, dynamics_constr, Q, R, Qf, freq, N, name=\"MPC_1\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc = Simulator(dynamics_constr, controller_mpc, env_constr, 1/freq, t_terminal)\n",
    "simulator_mpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc = Visualizer(simulator_mpc)\n",
    "visualizer_mpc.display_plots()\n",
    "visualizer_mpc.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0295b111",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Task 4: NMPC vs. ILQR**  \n",
    "\n",
    "In the following, we compare the performance of NMPC (Model Predictive Control for nonlinear systems) with the ILQR implementation introduced in Chapter 4. To this end, we introduce the following set of constraints:\n",
    "\n",
    "- Test case: case 4 (climbing up the hill)\n",
    "\n",
    "- State constraints: $ \\quad p \\in [-2.0, 2.0], \\quad v \\in [-4.0, 4.0],$  \n",
    "\n",
    "- Input constraints: $ \\quad a \\in [-8.0, 8.0],$  \n",
    "\n",
    "By introducing these constraints, we simulate the system's response and contrast the controllers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a4bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nonlinear case number\n",
    "case_nonlinear = 4\n",
    "\n",
    "# Define the physical boundary condition\n",
    "state_lbs = np.array([-2.0, -4.0])\n",
    "state_ubs = np.array([2.0, 4.0])\n",
    "input_lbs = -8.0\n",
    "input_ubs = 8.0\n",
    "\n",
    "# Define weight matrix in stage and terminal cost and the horizon for MPC \n",
    "Q = np.diag([10, 10])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8201bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env'\n",
    "env_nonlinear = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_nonlinear = Dynamics(env_nonlinear)\n",
    "# Instantiate the LQR controller class\n",
    "controller_lqr_nonlinear = LQRController(env_nonlinear, dynamics_nonlinear, Q, R, freq, name='LQR_case4_ideal', verbose=None)\n",
    "controller_lqr_nonlinear.set_lin_point(np.array([0.0, 0.0]))\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_lqr_nonlinear = Simulator(dynamics_nonlinear, controller_lqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_lqr_nonlinear.run_simulation()\n",
    "# Use trajectories getting from LQR to initialize ILQR\n",
    "_, input_traj_lqr_nonlinear = simulator_lqr_nonlinear.get_trajectories()\n",
    "\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear = iLQRController(env_nonlinear, dynamics_nonlinear, Q, R, Qf, freq, name='ILQR_case4_ideal', verbose=None)\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear.setup(input_traj_lqr_nonlinear)\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_ilqr_nonlinear = Simulator(dynamics_nonlinear, controller_ilqr_nonlinear, env_nonlinear, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear.run_simulation()\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "env_nonlinear_clipped = Env(case_nonlinear, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]), \n",
    "                           input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "# Instantiate the ILQR controller class\n",
    "controller_ilqr_nonlinear_clipped = iLQRController(env_nonlinear_clipped, dynamics_nonlinear, Q, R, Qf, freq, name='ILQR_case4_clipped')\n",
    "# Initialize ILQR controller with trajectory from LQR, then run setup function to compute optimal policy\n",
    "controller_ilqr_nonlinear_clipped.setup(input_traj_lqr_nonlinear)\n",
    "# Instantiate the simulator, and then run the simulation\n",
    "simulator_ilqr_nonlinear_clipped = Simulator(dynamics_nonlinear, controller_ilqr_nonlinear_clipped, env_nonlinear_clipped, 1/freq, t_terminal)\n",
    "simulator_ilqr_nonlinear_clipped.run_simulation()\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_ilqr_nonlinear_clipped = Visualizer(simulator_ilqr_nonlinear_clipped)\n",
    "visualizer_ilqr_nonlinear_clipped.display_contrast_plots(simulator_ilqr_nonlinear)\n",
    "visualizer_ilqr_nonlinear_clipped.display_contrast_animation_same(simulator_lqr_nonlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_nonlinear_constr = MPCController(env_nonlinear_clipped, dynamics_nonlinear, Q, R, Qf, freq, N, name=\"MPC_case4_constr\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_nonlinear_constr = Simulator(dynamics_nonlinear, controller_mpc_nonlinear_constr, env_nonlinear_clipped, 1/freq, t_terminal)\n",
    "simulator_mpc_nonlinear_constr.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_nonlinear_constr = Visualizer(simulator_mpc_nonlinear_constr)\n",
    "visualizer_mpc_nonlinear_constr.display_plots()\n",
    "visualizer_mpc_nonlinear_constr.display_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d0f3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show comparison between 2 cases\n",
    "visualizer_mpc_nonlinear_constr.display_contrast_plots(simulator_ilqr_nonlinear_clipped)\n",
    "visualizer_mpc_nonlinear_constr.display_contrast_animation_same(simulator_ilqr_nonlinear_clipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccffc3bc",
   "metadata": {},
   "source": [
    "#### **Results Analysis:**\n",
    "\n",
    "The simulation results reveal that, in this case, the ILQR-controlled system fails to be stabilized around the target state. This failure arises from the fact that equilibrium points in nonlinear systems generally do not exhibit global convergence. When actuator saturation occurs — typically due to clipping beyond the input bounds — the system trajectory may become trapped in an unstable region, rendering the controller ineffective.\n",
    "\n",
    "In contrast, NMPC explicitly incorporates input and state constraints into the optimization process, thereby avoiding unexpected saturation. Although the NMPC controller is subject to the same input constraints, it is able to leverage trajectory prediction over a finite horizon to plan ahead. Instead of attempting to reach the top in a single move—which is infeasible due to the limited input—it intentionally generates a control sequence that builds up momentum over time. This can be observed from the fact that the cart reaches a higher velocity when passing the starting point for the second time. As a result, despite the input limitations, the cart successfully climbs to the top after two rounds of back-and-forth motion, demonstrating the planner’s ability to exploit system dynamics over time.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c949",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "\n",
    "### **Part (d): Hyperparameter Selection**\n",
    "\n",
    "The main hyperparameters in MPC include the prediction horizon $N$, sampling frequency $f$, and weighting matrices $\\boldsymbol{Q}$, $\\boldsymbol{R}$, and $\\boldsymbol{Q}_f$, etc. The influence of these hyperparameters are as follows:\n",
    "\n",
    "  1) Prediction horizon $N$: A shorter prediction horizon reduces computation time per cycle, improving real-time compatibility, but may result in suboptimal or overly reactive behavior due to the limited foresight. A longer horizon allows the controller to anticipate future states more effectively, often producing smoother or more optimal trajectories, but at the cost of increased computational load and longer computation time per cycle;\n",
    "\n",
    "  2) Sampling frequency $f$: A higher sampling frequency (i.e., smaller sampling time) enables faster control updates and can predict the behavior of the high-frequency system dynamics more accurately, leading to better control performance, especially in fast systems. However, it increases the computational burden due to more frequent updates. Conversely, a lower sampling frequency reduces computational demand but may degrade performance or even destabilize the system if the controller cannot react sufficiently fast;\n",
    "\n",
    "  3) Weighting matrices $\\boldsymbol{Q}$, $\\boldsymbol{R}$, and $\\boldsymbol{Q}_f$: These matrices define the relative importance of state tracking versus control effort in the cost function. $\\boldsymbol{Q}$ penalizes deviation from desired states, $\\boldsymbol{R}$ penalizes excessive or aggressive control inputs, and $\\boldsymbol{Q}_f$ defines the terminal state penalty, encouraging the system to reach the final desired state smoothly. As in LQR, tuning these matrices allows you to balance tracking accuracy and control smoothness or energy efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecd5ec0",
   "metadata": {},
   "source": [
    "##### **Example: Prediction Horizon**\n",
    "\n",
    "As discussed in Chapter 2, the impact of the weight matrices on the performance of MPC has been well-established. The prediction horizon $N$, as another important and unique hyperparameter in the MPC algorithm, similarly plays a crucial role. Therefore, in this section, we will focus on exploring the impact of the horizon length on the performance of the MPC algorithm.\n",
    "\n",
    "To better highlight the impact of the prediction horizon, we specifically designed the following task setup:\n",
    "\n",
    "- Environment: case 4, varying slope, underactuated case (shape as shown in the output of the first block)\n",
    "\n",
    "- Initial/target state: $ \\quad \\boldsymbol{x}_s = [-0.5, 0]^T, \\quad \\boldsymbol{x}_T = [0.6, 0]^T,$  \n",
    "\n",
    "- State constraints: $ \\quad p \\in [-2.0, 2.0], \\quad v \\in [-4.0, 4.0],$  \n",
    "\n",
    "- Input constraints: $ \\quad a \\in [-5.0, 5.0],$  \n",
    "\n",
    "The hyperparameters are:\n",
    "\n",
    "- Weight for state $\\bm{Q} = \\text{diag}([1, 1])$ (requirement: symmetric, positive semi-definite matrix)  \n",
    "\n",
    "- Weight for input $\\bm{R} = [0.1]$ (requirement: symmetric, positive definite matrix)  \n",
    "    \n",
    "- Control frequency $f = 20$\n",
    "\n",
    "- Horizon $N = 20 / 60 / 100$ **(Control Group)**\n",
    "\n",
    "Based on the above task and controller setup, we instantiate various configurations and perform simulations. Vary the prediction horizon using values of 20, 60, and 100, respectively, and observe the corresponding simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the case index for the slope profile\n",
    "case = 4 \n",
    "\n",
    "# Define the initial and target state\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.6\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the physical boundary condition\n",
    "state_lbs = np.array([-2.0, -4.0])\n",
    "state_ubs = np.array([2.0, 4.0])\n",
    "input_lbs = -5.0\n",
    "input_ubs = 5.0\n",
    "\n",
    "# Instantiate class 'Env' and visualize the shape of the slope (left side) and theta curve (right side) \n",
    "#env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]))\n",
    "env = Env(case, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "env.test_env()\n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics = Dynamics(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5975915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight matrix in stage and terminal cost\n",
    "Q = np.diag([1, 1])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 6 # time length of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87100ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "# Define parameters of simulation\n",
    "freq = 20 # controll frequency\n",
    "t_terminal = 6 # time length of simulation\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_N20 = MPCController(env, dynamics, Q, R, Qf, freq, N, name=\"MPC_N20\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_N20 = Simulator(dynamics, controller_mpc_N20, env, 1/freq, t_terminal)\n",
    "simulator_mpc_N20.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_N20 = Visualizer(simulator_mpc_N20)\n",
    "visualizer_mpc_N20.display_plots(title='N = 20')\n",
    "visualizer_mpc_N20.display_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_N60 = MPCController(env, dynamics, Q, R, Qf, freq, N, name=\"MPC_N60\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_N60 = Simulator(dynamics, controller_mpc_N60, env, 1/freq, t_terminal)\n",
    "simulator_mpc_N60.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_N60 = Visualizer(simulator_mpc_N60)\n",
    "visualizer_mpc_N60.display_plots(title='N = 60')\n",
    "visualizer_mpc_N60.display_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96adac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "\n",
    "# Instantiate the MPC controller class (controller name must be different from the rest controllers)\n",
    "controller_mpc_N100 = MPCController(env, dynamics, Q, R, Qf, freq, N, name=\"MPC_N100\")\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_N100 = Simulator(dynamics, controller_mpc_N100, env, 1/freq, t_terminal)\n",
    "simulator_mpc_N100.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_N100 = Visualizer(simulator_mpc_N100)\n",
    "visualizer_mpc_N100.display_plots(title='N = 100')\n",
    "visualizer_mpc_N100.display_animation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e2cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show comparison between all cases\n",
    "visualizer_mpc_N20.display_contrast_plots(simulator_mpc_N60, simulator_mpc_N100)\n",
    "visualizer_mpc_N20.display_contrast_animation_same(simulator_mpc_N60, simulator_mpc_N100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671287ca",
   "metadata": {},
   "source": [
    "##### **Results Analysis: System Response Under Different Horizon Lengths**\n",
    "\n",
    "For the defined task, while keeping the value of all other hyperparameters fixed, we vary the value of $N$ to represent short, medium, and long horizon cases. From the experimental results, the following trends can be observed:\n",
    "\n",
    "1. **Short prediction horizon** ($N = 20$):  \n",
    "   - Under this condition, the car is unable to reach the target position. Instead, after a brief oscillation, it comes to rest at the bottom of the valley;  \n",
    "   - This outcome arises because, within the current prediction horizon (20 steps at 20 Hz, i.e., 1 second), the presence of input constraints makes it impossible to reach the goal even under a bang-bang control strategy. Consequently, the MPC prioritizes minimizing the penalty term on input, leading to a zero-input solution.\n",
    "\n",
    "2. **Medium prediction horizon** ($N = 60$):  \n",
    "   - In this scenario, the car begins by moving backward, undergoes cycles of back-and-forth motion with increasing amplitude, and then propels itself toward the top, successfully reaching the target location;\n",
    "   - This behavior arises because, in the underactuated case, the car is unable to generate sufficient input to reach the top in a single ascent. However, when the prediction horizon is sufficiently long, the system can perform periodic back-and-forth motions to accumulate additional momentum. Once enough kinetic energy has been gained, the car is able to propel itself to the top.\n",
    "\n",
    "3. **Long prediction horizon** ($N = 100$):  \n",
    "   - In this case, compared to the medium prediction horizon scenario, the car requires fewer cycles to accumulate enough momentum accumulation to reach the top;\n",
    "   - This is because, compared to the case with N=60, the prediction horizon with N=100 covers a longer time interval. Specifically, N=60 corresponds to a prediction window of 3s (60 steps at 20 Hz), while the transient behavior in this case lasts approximately 4 seconds, as observed from the simulation results. As a result, at the start of the optimization, the MPC cannot foresee the car reaching the top, and thus chooses a strategy involving more cycles of momentum accumulation. \n",
    "   - On the other hand, with N=100, the prediction horizon spans 5 seconds. Therefore, the prediction horizon fully encompasses the transient behavior, allowing the MPC to effectively optimize over approximately the whole timeline. As a result, the computed control sequence can be regarded as a near-optimal policy.\n",
    "\n",
    "##### **Main Conclusion:**\n",
    "\n",
    "In summary, the length of the prediction horizon significantly influences the system's ability to generate effective control strategies. A short horizon limits the controller's foresight, leading to suboptimal or even ineffective behavior. In contrast, a sufficiently long horizon allows the MPC to anticipate the full transient dynamics and compute near-optimal control actions. Therefore, selecting an appropriate horizon length is critical for achieving desired performance in MPC design.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
