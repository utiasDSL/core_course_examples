{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3b17c7",
   "metadata": {},
   "source": [
    "\n",
    "### **Chapter 6.2: Learning-based Model Predictive Control**\n",
    "\n",
    "In this chapter, we introduce the motivation and structure of learning-based model predictive control (MPC) under uncertainty. When the true system dynamics are unknown or difficult to model exactly, we rely on data-driven approaches to build approximate models and design robust controllers.\n",
    "\n",
    "We start by setting up a bumpy terrain case as the simulation environment, and specifying all task parameters. This provides a unified testbed for comparing different control strategies. Then we provide auxiliary functions for basis generation and symbolic model construction. These tools are essential for flexible model learning and controller synthesis. Based on all these preparations, we can develop different controllers and run the simulation according to different level of prior knowledges we know about the real environment, which proceeds through several key stages:\n",
    "\n",
    "- **Baseline 1:** Simulate MPC designed with the true model (bumpy case) to establish a baseline for performance benchmark.\n",
    "\n",
    "- **Baseline 2:** Simulate MPC designed with a mismatched model (flat case) and compare with the first example to illustrate the impact of model mismatch.\n",
    "\n",
    "- **Deterministic learning:** Collect data, identify a deterministic model using **Linear Regression**, and simulate MPC designed with the learned model to demonstrate data-driven control without uncertainty quantification.\n",
    "\n",
    "- **Probabilistic learning (Robust MPC):** Assume a mismatched model (flat case) and uncertainty bounds and implement **robust MPC (RMPC)** to handle model uncertainty conservatively.\n",
    "\n",
    "- **Probabilistic learning (Bayesian):** Collect data, identify a probabilistic model using **Bayesian Linear Regression**, and simulate **Learning-based MPC (LBMPC)** with uncertainty quantification, enabling constraint tightening and risk-aware control.\n",
    "\n",
    "By the end of this chapter, you will understand how to formulate, implement, and analyze both deterministic and probabilistic model learning techniques, and how they integrate into advanced MPC frameworks for uncertain systems.\n",
    "\n",
    "All the contents are summarized in the table below.  \n",
    "\n",
    "<table border=\"1\" style=\"border-collapse: collapse; text-align: center;\">\n",
    "  <!-- Title Row -->\n",
    "  <tr>\n",
    "    <th colspan=\"2\" style=\"text-align:center\">Content of Chapter 6.2 Exercise</th>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 2 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Part 1: Baselines without Uncertainty Handling</td>\n",
    "    <td>Example 1.1: Baseline MPC under Perfect Model Assumption</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 1.2: MPC under Model Mismatch with Flat Terrain Assumption</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 3 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"1\">Part 2: Deterministic Model Learning and Control</td>\n",
    "    <td>Example 2.1: MPC with Learned Deterministic Model from Data</td>\n",
    "  </tr>\n",
    "\n",
    "  <!-- Row group 4 -->\n",
    "  <tr>\n",
    "    <td rowspan=\"2\">Part 3: Probabilistic Model Learning and Control</td>\n",
    "    <td>Example 3.1: Robust MPC with Flat Model and Bounded Uncertainty</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Example 3.2: Learning-Based MPC via Bayesian Model Identification</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "\n",
    "First, we need to set up our Python environment and import relevant packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e187b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from acados_template import  AcadosModel, AcadosOcp, AcadosOcpSolver\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from utils.env import *\n",
    "from utils.simulator import *\n",
    "from ex2_LQR.lqr_utils import *\n",
    "from ex5_MPC.mpc_utils import *\n",
    "from ex6_SysID.SysID_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7f2fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Experimental Setup and Unified Benchmark for Uncertain Terrain Control**\n",
    "\n",
    "In this chapter, we address the challenge of controlling a mountain car system when the true environment is uncertain and potentially complex. Specifically, we assume that the real testbed is a bumpy terrain $h(p) = k \\cdot \\cos(18 p)$ (as case 3 defined before), but as a controller designer, you do not know the exact profile of the terrain. Instead, you are aware that there is some uncertainty in the system dynamics, and your goal is to design controllers that can handle this uncertainty effectively.\n",
    "\n",
    "To systematically study and compare different control strategies, we establish a unified simulation testbed. This testbed allows us to specify all relevant task parameters and use them consistently across all examples in the chapter. By doing so, we can fairly evaluate the performance of various approaches, from ideal model-based control to learning-based and robust controllers.\n",
    "\n",
    "- Parameters of simulation:  \n",
    "\n",
    "   - Case index of testbed: case 3 (bumpy terrain) with parameter $k = 0.01$\n",
    "\n",
    "   - Control frequency: 10 Hz \n",
    "\n",
    "   - Simulation time: 8 seconds\n",
    "\n",
    "- Parameters of task:  \n",
    "\n",
    "   - initial state: $\\boldsymbol{x}_0 = [-0.5, 0.0]^T$\n",
    "\n",
    "   - target state: $\\boldsymbol{x}_T = [0.5, 0.0]^T$\n",
    "\n",
    "   - State constraints: $ \\quad p \\in [-2.0, 2.0], \\quad v \\in [-4.0, 4.0],$  \n",
    "\n",
    "   - Input constraints: $ \\quad a \\in [-8.0, 8.0],$  \n",
    "\n",
    "- Parameters of controller:  \n",
    "\n",
    "   - Prediction horizon: $N = 20$\n",
    "   \n",
    "   - State cost matrix: $\\boldsymbol{Q} = diag([5, 5])$  \n",
    "\n",
    "   - Input cost matrix: $\\boldsymbol{R} = 0.1$  \n",
    "\n",
    "   - Terminal cost matrix: $\\boldsymbol{Q}_f = \\boldsymbol{Q}$ \n",
    "\n",
    "\n",
    "These parameters define the environment and control objectives for all subsequent simulations and experiments in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb6f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the test case\n",
    "case_real = 3\n",
    "\n",
    "# Define the parameters of the slope\n",
    "terrain_param = 0.01\n",
    "\n",
    "# Define task parameters\n",
    "initial_position = -0.5\n",
    "initial_velocity = 0.0\n",
    "target_position = 0.5\n",
    "target_velocity = 0.0\n",
    "\n",
    "# Define the physical boundary condition\n",
    "state_lbs = np.array([-2.0, -4.0])\n",
    "state_ubs = np.array([2.0, 4.0])\n",
    "input_lbs = -8.0\n",
    "input_ubs = 8.0\n",
    "\n",
    "# Define the parameters in the controller\n",
    "N = 20\n",
    "Q = np.diag([5, 5])\n",
    "R = np.array([[0.1]])\n",
    "Qf = Q\n",
    "\n",
    "# Define parameters for simulation\n",
    "freq = 10 # controll frequency\n",
    "t_terminal = 8 # time length of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41da6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate class 'Env'\n",
    "env_real = Env(case_real, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "          param=terrain_param, state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "env_real.test_env()\n",
    "\n",
    "# Instantiate class 'Dynamics'\n",
    "dynamics_real = Dynamics(env_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b333de",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Auxiliary Functions for Generating the Symbolic Expression of Identifed Model**\n",
    "\n",
    "The first utility function **`generate_basis_functions`** simplifies the construction of basis sets for regression by allowing you to automatically generate combinations of polynomial and trigonometric functions with configurable orders and frequencies.  \n",
    "\n",
    "**Arguments:**  \n",
    "\n",
    "- `include_offset`: Whether to include the constant bias term $ \\phi(p) = 1 $\n",
    "\n",
    "- `include_polynomial`: Whether to include polynomial terms $ p^i $\n",
    "\n",
    "- `include_trigonometric`: Whether to include trigonometric terms $ \\sin(k p) $, $ \\cos(k p) $\n",
    "\n",
    "- `min_poly_order`, `max_poly_order`: Range of polynomial degrees $ i $\n",
    "\n",
    "- `min_freq`, `max_freq`: Range of trigonometric frequencies $ k $\n",
    "\n",
    "- `casadi_sym`: Whether to generate CasADi symbolic expressions of the basis functions\n",
    "\n",
    "**Returns:**  \n",
    "\n",
    "- `basis`: List of callable basis functions $\\{ \\phi_1(p), ... , \\phi_B(p) \\}$. Each basis function accepts a NumPy array of shape `(D, 1)` or `(D,)` and returns a transformed column vector of shape `(D, 1)`.\n",
    "\n",
    "- `p_sym`: Symbolic variable (later used in construnction of the symbolic identified model)  \n",
    "\n",
    "- `basis_syms`: List of symbolic expressions of the basis functions $\\{ \\phi_1(p), ... , \\phi_B(p) \\}$ (later used in construnction of the symbolic identified model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d069304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basis_functions(include_offset=True,\n",
    "                              include_polynomial=True,\n",
    "                              include_trigonometric=True,\n",
    "                              min_poly_order=1,\n",
    "                              max_poly_order=10,\n",
    "                              min_freq=1,\n",
    "                              max_freq=10,\n",
    "                              casadi_sym=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate a list of basis functions for use in linear regression or Bayesian regression.\n",
    "\n",
    "    Parameters:\n",
    "    - include_offset: whether to include the constant 1 term\n",
    "    - include_polynomial: whether to include polynomial terms p^i\n",
    "    - include_trigonometric: whether to include sin(kp) and cos(kp) terms\n",
    "    - min_poly_order: lowest polynomial degree to include\n",
    "    - max_poly_order: highest polynomial degree to include\n",
    "    - min_freq: lowest frequency k for trig functions\n",
    "    - max_freq: highest frequency k for trig functions\n",
    "    - casadi_sym: if set to be True, use this symbolic variable for generating symbolic basis functions (basde on CasADi)\n",
    "\n",
    "    Returns:\n",
    "    - basis: a list of callable functions, each \\\\phi(\\\\xi) taking \\\\xi as (D, 1) or (D,) array\n",
    "    - basis_syms: a list of symbolic expressions for each basis function, if casadi_sym is provided, else None\n",
    "    \"\"\"\n",
    "    \n",
    "    basis = []\n",
    "    # If using symbolic computation with CasADi, initialize symbolic variables\n",
    "    if casadi_sym:\n",
    "        p_sym = ca.MX.sym(\"p\")\n",
    "        basis_syms = []\n",
    "    else:\n",
    "        basis_syms = None\n",
    "\n",
    "    # Offset term\n",
    "    if include_offset:\n",
    "        basis.append(lambda p: np.ones_like(p))\n",
    "        # If using symbolic computation with CasADi, add the symbolic constant\n",
    "        if casadi_sym:\n",
    "            basis_syms.append(1)\n",
    "\n",
    "    # Polynomial terms\n",
    "    if include_polynomial:\n",
    "        for i in range(min_poly_order, max_poly_order + 1):\n",
    "            basis.append(lambda p, i=i: p**i)\n",
    "        # If using symbolic computation with CasADi, add the symbolic polynomial terms\n",
    "        if casadi_sym:\n",
    "            for i in range(min_poly_order, max_poly_order + 1):\n",
    "                basis_syms.append(p_sym ** i)\n",
    "\n",
    "    # Trigonometric terms\n",
    "    if include_trigonometric:\n",
    "        for k in range(min_freq, max_freq + 1):\n",
    "            basis.append(lambda p, k=k: np.sin(k * p))\n",
    "            basis.append(lambda p, k=k: np.cos(k * p))\n",
    "        # If using symbolic computation with CasADi, add the symbolic trigonometric terms\n",
    "        if casadi_sym:\n",
    "            for k in range(min_freq, max_freq + 1):\n",
    "                basis_syms.append(ca.sin(k * p_sym))\n",
    "                basis_syms.append(ca.cos(k * p_sym))\n",
    "\n",
    "    return basis, p_sym, basis_syms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5057a32f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The second function **`construct_casadi_expression`** constructs a CasADi symbolic function for the learned model, combining basis functions and regression weights. It can also provide a function for the predictive variance if uncertainty information is available.  \n",
    "\n",
    "**Arguments:**  \n",
    "\n",
    "- `p_sym`: CasADi symbolic variable of independent variable $p$ \n",
    "\n",
    "- `basis_syms`: List of symbolic basis expressions $\\{ \\phi_1(p), ... , \\phi_B(p) \\}$\n",
    "\n",
    "- `theta`: Regression weights $\\hat{\\mu}_\\theta$ (attained from **LR** or **BLR**)\n",
    "\n",
    "- `Sigma_theta` (only for **BLR**): Covariance of weights $\\hat{\\Sigma}_\\theta$ (attained from **BLR**)\n",
    "\n",
    "- `sigma2_prior` (only for **BLR**): Prior noise variance $\\sigma^2$\n",
    "\n",
    "**Returns:**  \n",
    "\n",
    "- `h_func`: CasADi function for the mean prediction $\\hat{\\mu}_{h}(p^*) = \\hat{\\mu}_\\theta^\\top \\phi(p^*)$\n",
    "\n",
    "- `sigma2_h` (only for **BLR**): CasADi function for the predictive variance matirx $\\hat{\\sigma}^2_{h}(p^*) = \\sigma^2 + \\phi(p^*)^\\top \\hat{\\Sigma}_\\theta \\phi(p^*)$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f94abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_casadi_expression(p_sym, basis_syms, theta, Sigma_theta=None, sigma2_prior=None):\n",
    "    \"\"\"\n",
    "    Construct a CasADi symbolic function for h(p) = sum_i theta_i * phi_i(p)\n",
    "\n",
    "    Parameters:\n",
    "    - p_sym: CasADi symbolic variable representing the input\n",
    "    - theta: numpy array of shape (D, 1) or (D,) from Identifier_LR\n",
    "    - basis_syms: list of CasADi symbolic expressions for each basis function\n",
    "    - Sigma_theta: optional covariance matrix for Bayesian regression (D, D)\n",
    "    - sigma2_prior: optional prior variance for Bayesian regression (scalar)\n",
    "\n",
    "    Returns:\n",
    "    - h_expr: CasADi symbolic expression\n",
    "    - p_sym: CasADi symbolic variable\n",
    "    - h_func: CasADi Function that maps p → h(p)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure basis_syms is concatenated into a vector expression\n",
    "    Phi_sym = ca.vertcat(*basis_syms)\n",
    "\n",
    "    # Compute h(p) expression\n",
    "    h_expr = ca.dot(Phi_sym, theta)  # same as sum(theta[i] * phi_i)\n",
    "\n",
    "    # Define CasADi function for mean\n",
    "    h_func = ca.Function(\"h_post\", [p_sym], [h_expr])\n",
    "\n",
    "    # Define CasADi function for variance if required\n",
    "    if Sigma_theta is not None and sigma2_prior is not None:\n",
    "        Sigma_expr = ca.mtimes([Phi_sym.T, ca.DM(Sigma_theta), Phi_sym]) + ca.DM(sigma2_prior)\n",
    "        sigma2_h = ca.Function(\"sigma2_h_post\", [p_sym], [Sigma_expr])\n",
    "    else:\n",
    "        sigma2_h = None\n",
    "\n",
    "    return h_func, sigma2_h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ac39e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part 1: Baselines without Uncertainty Handling**\n",
    "\n",
    "To understand the necessity of handling model uncertainty in control design, we begin by establishing baseline scenarios where uncertainty is either absent or ignored. In this part, we first examine the ideal case where the controller has full knowledge of the true environment, serving as an upper bound for performance. We then contrast this with a naive design that assumes a simplified, mismatched model, revealing the degradation in performance that arises when uncertainty is neglected. These baselines provide valuable context for evaluating the benefits of more sophisticated, uncertainty-aware controllers in subsequent sections.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Example 1.1: Baseline MPC under Perfect Model Assumption**\n",
    "\n",
    "In this example, we assume full knowledge of the true terrain profile. We simulate MPC designed with the true model (case 3) to establish a baseline for performance benchmark. This serves as the ideal scenario, providing a reference for evaluating the impact of model mismatch and learning-based approaches in subsequent examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7100d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPC controller\n",
    "controller_mpc_ideal = MPCController(env_real, dynamics_real, Q, R, Qf, freq, N, name='MPC_baseline', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_ideal = Simulator(dynamics_real, controller_mpc_ideal, env_real, 1/freq, t_terminal)\n",
    "simulator_mpc_ideal.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_ideal = Visualizer(simulator_mpc_ideal)\n",
    "visualizer_mpc_ideal.display_plots(\"Baseline MPC under Perfect Model Assumption\")\n",
    "visualizer_mpc_ideal.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42eec2",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 1.2: MPC under Model Mismatch with Flat Terrain Assumption**\n",
    "\n",
    "In this example, we consider the scenario where the controller designer does not have access to the true terrain profile. Instead, the designer assumes a **simplified, flat terrain model** (case 1) for controller synthesis. We simulate MPC designed with this mismatched (flat) model and apply it to the real bumpy environment. By comparing the results with the ideal baseline (where the true model is known), we can clearly illustrate the impact of model mismatch on control performance and highlight the necessity of accounting for model uncertainty in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c9c982",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_ideal = 1 # 1 or 2 or 3 or 4\n",
    "\n",
    "# Ideal case for controller design\n",
    "env_ideal = Env(case_ideal, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 param=terrain_param, state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_ideal = Dynamics(env_ideal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPC controller\n",
    "controller_mpc_real = MPCController(env_ideal, dynamics_ideal, Q, R, Qf, freq, N, name='MPC_flat_terrain_model', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_real = Simulator(dynamics_real, controller_mpc_real, env_real, 1/freq, t_terminal)\n",
    "simulator_mpc_real.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_real = Visualizer(simulator_mpc_real)\n",
    "visualizer_mpc_real.display_plots(\"MPC under Model Mismatch with Flat Terrain Assumption\")\n",
    "visualizer_mpc_real.display_animation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433cc0e",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "In the ideal case without model mismatch, the system accurately follows the predicted trajectory and successfully reaches the target state with smooth and efficient control. In contrast, with model mismatch, the predicted and actual states deviate noticeably—especially in position—and the system fails to converge to the target. The control inputs become more aggressive, highlighting the negative impact of model inaccuracies on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfe531b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part 2: Deterministic Model Learning and Control**\n",
    "\n",
    "In this part, we consider the scenario where the true system dynamics are unknown, but we have the ability to collect data from the environment and use it to learn an approximate deterministic model. We demonstrate how to use linear regression with carefully chosen basis functions to identify the terrain profile, and then synthesize an MPC controller based on the learned model. This section highlights the workflow of data-driven control, showing how model identification from data can enable effective control even without perfect prior knowledge of the system.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Example 2.1: MPC with Learned Deterministic Model from Data**\n",
    "\n",
    "We proceed by first collecting input-output samples from the real environment (case 3 with a specified terrain parameter $k$), where the measurements are affected by additive Gaussian noise $w_k \\sim \\mathcal{N}(0, \\sigma^2 I)$. This dataset serves as the basis for model identification. To flexibly approximate the unknown terrain profile, we select a set of basis functions consisting of trigonometric terms—specifically sine and cosine functions with frequencies ranging from 16 to 20—and include a constant offset. Polynomial terms are excluded to better capture the periodic nature of the terrain. Using a standard Linear Regression (LR) approach, we fit the collected data to these basis functions, yielding a deterministic approximation of the terrain. Finally, this learned model is embedded into the MPC framework and evaluated in the true environment to assess its closed-loop performance.\n",
    "\n",
    "- **Reference Arguments for Basis function configuration:**\n",
    "\n",
    "    - Include offset: `True`\n",
    "\n",
    "    - Include polynomial terms: `False`\n",
    "\n",
    "    - Include trigonometric terms: `True`\n",
    "\n",
    "    - Minimum frequency: `16`\n",
    "\n",
    "    - Maximum frequency: `20`\n",
    "\n",
    "- **Reference Arguments for Data generation:**\n",
    "\n",
    "    - Number of samples: `100`\n",
    "\n",
    "    - Measurement noise: zero-mean Gaussian, standard deviation $\\sigma$ as `0.005`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define basis functions\n",
    "include_offset=True\n",
    "include_polynomial=False\n",
    "include_trigonometric=True\n",
    "min_poly_order=1\n",
    "max_poly_order=1\n",
    "min_freq=16\n",
    "max_freq=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc97587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data\n",
    "np.random.seed(49) # for reproducibility\n",
    "data_gen = GenerateData(num_samples=100, case=case_real, param=terrain_param)\n",
    "data_gen.set_noise(mean=0.0, std=0.005) # (I.I.D.) zero-mean Gaussian noise\n",
    "p, h = data_gen.generate_data()\n",
    "\n",
    "\n",
    "# Generate basis functions\n",
    "basis, p_sym, basis_sym = generate_basis_functions(include_offset, include_polynomial, include_trigonometric, \n",
    "                                                   min_poly_order, max_poly_order, min_freq, max_freq)\n",
    "\n",
    "# Fit the model using linear regression\n",
    "model_lr = Identifier_LR(basis)\n",
    "model_lr.fit(p, h)\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "true_func = data_gen.get_symbolic_function()\n",
    "model_lr.plot(true_func=true_func, title=\"Learned Terrain Model via Linear Regression\")\n",
    "\n",
    "\n",
    "# Generate the symbolic expression for the learned model\n",
    "h_func_lr, _ = construct_casadi_expression(p_sym, basis_sym, model_lr.theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef136c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real case for simulation\n",
    "env_id = Env(case_real, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 symbolic_h_mean_ext=h_func_lr, param=terrain_param, \n",
    "                 state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_id = Dynamics(env_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f62c658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MPC controller\n",
    "controller_mpc_identified = MPCController(env_id, dynamics_id, Q, R, Qf, freq, N, name='MPC_identified', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_mpc_identified = Simulator(dynamics_real, controller_mpc_identified, env_real, 1/freq, t_terminal)\n",
    "simulator_mpc_identified.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_mpc_identified = Visualizer(simulator_mpc_identified)\n",
    "visualizer_mpc_identified.display_contrast_plots(simulator_mpc_ideal, simulator_mpc_real, title=\"Performance Comparison of MPC Controllers under Different Model Assumptions\")\n",
    "visualizer_mpc_identified.display_contrast_animation_same(simulator_mpc_ideal, simulator_mpc_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c75629",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "The simulation result highlights the performance of the MPC controller using an identified model (blue) compared to the ideal model (red) and a mismatched flat model (green). The identified controller achieves nearly the same tracking accuracy and control efficiency as the ideal case, demonstrating that a well-learned model from data can effectively recover high performance. In contrast, the flat model leads to steady-state error and larger inputs due to model mismatch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74d307e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "### **Part 3: Probabilistic Model Learning and Control**\n",
    "\n",
    "In this section, we address the challenge of controlling systems under uncertainty by leveraging probabilistic model learning and robust control strategies. Unlike previous parts where either the true model or a deterministic learned model was used, here we explicitly account for model uncertainty—either by assuming bounded disturbances or by learning a probabilistic model from data. We first demonstrate robust MPC (RMPC) using a mismatched flat model with relatively large uncertainty bounds, and then introduce Bayesian Linear Regression to quantify model uncertainty and enable Learning-Based MPC (LBMPC) with constraint tightening.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Example 3.1: Robust MPC with Flat Model and Bounded Uncertainty**\n",
    "\n",
    "In this example, we assume the controller designer does not know the true terrain profile and instead simplifies it as flat. However, unlike previous cases that ignore the modeling error, here we explicitly consider that the mismatch between the flat model and reality is bounded. We specify a box-type disturbance set with:\n",
    "\n",
    "$$\n",
    "w_1 \\in \\mathcal{D}_1 = [-0.4, 0.4], \\quad w_2 \\in \\mathcal{D}_2 = [-0.8, 0.8]\n",
    "$$\n",
    "\n",
    "representing prior belief on how much the unmodeled dynamics can deviate. With this uncertainty set, we design a **Robust MPC (RMPC)** controller that tightens the constraints based on worst-case disturbance propagation. This ensures that safety constraints are satisfied at all times, even in the presence of the assumed disturbance. The resulting controller is then evaluated in the real bumpy environment to assess performance under model uncertainty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bounds for the disturbances\n",
    "disturbance_lbs = np.array([-0.4, -0.8])\n",
    "disturbance_ubs = np.array([0.4, 0.8])\n",
    "\n",
    "# Define the RMPC controller\n",
    "controller_rmpc = LinearRMPCController(env_ideal, dynamics_ideal, Q, R, Qf, freq, N, \n",
    "                                       disturbance_bounds=[disturbance_lbs, disturbance_ubs], max_iter=10, name = 'RMPC_flat_terrain_model', verbose=False)\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_rmpc = Simulator(dynamics_real, controller_rmpc, env_real, 1/freq, t_terminal)\n",
    "simulator_rmpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_rmpc = Visualizer(simulator_rmpc)\n",
    "visualizer_rmpc.display_phase_portrait()\n",
    "visualizer_rmpc.display_plots(\"Performance of Robust MPC under Bounded Model Uncertainty\")\n",
    "visualizer_rmpc.display_animation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4d9c43",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "In this example, the nominal state trajectory successfully converges to the target position, demonstrating that the RMPC optimization works as intended under the assumed flat model. However, due to the presence of the robust invariant tube, the true state is only guaranteed to stay within a neighborhood around the nominal trajectory. Since the stabilizing feedback controller was designed based on the flat terrain dynamics, it may not generate sufficient corrective inputs on the actual sloped terrain to drive the true state exactly to the target. As a result, we observe a steady-state error in the true trajectory. This illustrates a key limitation of RMPC: while it ensures constraint satisfaction and robustness, it may fail to achieve precise convergence in the presence of unmodeled dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17925d",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### **Example 3.2: Learning-Based MPC via Bayesian Model Identification**\n",
    "\n",
    "In this example, we address the problem of controlling a system when the true environment is unknown and potentially complex, such as a bumpy terrain profile. Instead of assuming a fixed disturbance bound (as in RMPC), we collect data and apply **Bayesian Linear Regression (BLR)** to learn a **probabilistic model** of the terrain, capturing both the mean and the uncertainty of the terrain profile $ h(p) $. This learned probabilistic model is then integrated into an MPC framework. The core idea is to use the **mean prediction** of the BLR model to correct the dynamics and use the **predicted variance** to perform **constraint tightening**, ensuring that state constraints are satisfied with high probability.\n",
    "\n",
    "We begin by assuming the system dynamics can be written as:\n",
    "\n",
    "$$\n",
    "\\dot{x} = \\bar{f}(x, u) + \\mu^{\\text{LM}}(x, u) + w\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $ \\bar{f}(x, u) $: prior nominal dynamics,\n",
    "\n",
    "- $ \\mu^{\\text{LM}}(x, u) $: learned mean correction from the BLR model,\n",
    "\n",
    "- $ w \\sim \\mathcal{N}(0, \\Sigma^w) $: zero-mean Gaussian process noise.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;*Note that: here we do not use a prior model, but directly identify the whole dynamcis, i.e.: $\\bar{f}(x, u) = 0$ .*\n",
    "\n",
    "To implement Learning-Based MPC, we discretize the above and define the predicted mean state as:\n",
    "\n",
    "$$\n",
    "\\mu^{x}_{k+i+1|k} = \\bar{f}(\\mu^{x}_{k+i|k}, u_{k+i|k}) + \\mu^{\\text{LM}}(\\mu^{x}_{k+i|k}, u_{k+i|k})\n",
    "$$\n",
    "\n",
    "This gives us the **deterministic state prediction** based on both prior knowledge and the learned mean correction.\n",
    "\n",
    "\n",
    "To safely handle constraints under uncertainty, we **propagate the state covariance** across the prediction horizon. Using first-order uncertainty propagation, the covariance is updated as:\n",
    "\n",
    "$$\n",
    "\\Sigma^{x}_{k+i+1|k} = \n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\nabla^{\\top}_{x} \\bar{f}(\\mu^{x}_{k+i|k}, u_{k+i|k}) & \\mathbf{I}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "\\Sigma^{x}_{k+i|k} & \\star \\\\\n",
    "\\nabla_{x} \\mu^{\\text{LM}} (\\mu^{x}_{k+i|k}, u_{k+i|k}) \\Sigma^{x}_{k+i|k}  & \\Sigma^{\\text{LM}}(\\mu^{x}_{k+i|k}, u_{k+i|k}) + \\Sigma^{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "\\nabla_{x} \\bar{f}(\\mu^{x}_{k+i|k}, u_{k+i|k}) \\\\\n",
    "\\mathbf{I}\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where $ \\Sigma^{\\text{LM}} $ is the learned covariance from BLR.\n",
    "\n",
    "\n",
    "We then use this covariance to perform **constraint tightening**. Let the original state constraint set be $ \\mathcal{X} $. Consider the uncertainty set $\\mathcal{R}^x$ as:\n",
    "\n",
    "$$\n",
    "\\mathcal{R}^x = \\left\\{ x \\in \\mathbb{R}^n : |[e^x_i]|_j < r \\sqrt{[\\Sigma^x]_{j,j}}, \\quad \\forall j = 1, \\dots, n \\right\\}\n",
    "$$\n",
    "\n",
    "which defines a high-probability ellipsoidal confidence region around the predicted mean. The final **tightened constraint** for use in MPC is:\n",
    "\n",
    "$$\n",
    "\\mu^x_{k+i|k} \\in \\mathcal{X} \\ominus \\mathcal{R}^x\n",
    "$$\n",
    "\n",
    "This ensures that the true state lies within the original constraint set with high probability (e.g., $ p_x = 0.997 $ when $ r = 3 $).\n",
    "\n",
    "\n",
    "Finally, we formulate the deterministic LBMPC problem as:\n",
    "\n",
    "$$\n",
    "\\min_{u_{k|k}, \\dots, u_{k+N-1|k}} \\ell_N(\\mu^x_{k+N|k}) + \\sum_{i=0}^{N-1} \\ell_i(\\mu^x_{k+i|k}, u_{k+i|k})\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$$\n",
    "\\mu^{x}_{k+i+1|k} = \\bar{f}(\\mu^x_{k+i|k}, u_{k+i|k}) + \\mu^{\\text{LM}}(\\mu^x_{k+i|k}, u_{k+i|k}), \\quad \\forall i = \\{0, 1, ..., N-1\\},\n",
    "$$\n",
    "$$\n",
    "\\Sigma^{x}_{k+i+1|k} \\leftarrow \\text{ updated via the first-order approximation}\n",
    "$$\n",
    "$$\n",
    "\\mu^x_{k+i|k} \\in \\mathcal{X} \\ominus \\mathcal{R}^x(\\Sigma^x_{k+i|k}), \\quad \\forall i = \\{0, 1, ..., N-1\\},\n",
    "$$\n",
    "$$\n",
    "u_{k+i|k} \\in \\mathcal{U}, \\quad \\forall i = \\{0, 1, ..., N-1\\},\n",
    "$$\n",
    "$$\n",
    "\\mu^x_{k|k} = x_k, \\quad \\Sigma^x_{k|k} = 0.\n",
    "$$\n",
    "\n",
    "\n",
    "Through this setup, **LBMPC** ensures that both prediction and control account for model uncertainty, leading to improved safety and reduced conservatism compared to RMPC. The learned uncertainty from BLR drives the tightening, allowing a **data-efficient and probabilistically safe** control solution. We implement this data-driven controller as the class following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb19164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningBasedMPCController(LQRController):\n",
    "    def __init__(self, \n",
    "                 env: Env, \n",
    "                 dynamics: Dynamics, \n",
    "                 Q: np.ndarray, \n",
    "                 R: np.ndarray, \n",
    "                 Qf: np.ndarray, \n",
    "                 N: int, \n",
    "                 freq: float, \n",
    "                 beta: float = 3.0, \n",
    "                 Sigma_w: np.ndarray = None, \n",
    "                 name: str = 'LBMPC',\n",
    "                 type: str = 'MPC', \n",
    "                 verbose: bool = True):\n",
    "\n",
    "        self.Qf = Qf\n",
    "        self.N = N\n",
    "        self.beta = beta # default for beta is 3.0, which corresponds to 3-sigma rule\n",
    "\n",
    "        super().__init__(env, dynamics, Q, R, freq, name, type, verbose)\n",
    "\n",
    "        if self.env.h_cov is None:\n",
    "            raise ValueError(\"Dynamics variance function must be provided for LBMPC.\")\n",
    "        else:\n",
    "            self.dynamics.build_stochastic_model(self.dt)\n",
    "\n",
    "        self.Sigma_w = Sigma_w if Sigma_w is not None else np.zeros((self.dim_states, self.dim_inputs))\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        ## Model\n",
    "        # Set up Acados model\n",
    "        model = AcadosModel()\n",
    "        model.name = self.name\n",
    "\n",
    "        # Define model: x_dot = f(x, u)\n",
    "        model.x = self.dynamics.states\n",
    "        model.u = self.dynamics.inputs\n",
    "        model.f_expl_expr = ca.vertcat(self.dynamics.dynamics_function(self.dynamics.states, self.dynamics.inputs))\n",
    "        model.f_impl_expr = None # no needed, we already have the explicit model\n",
    "\n",
    "        ocp = AcadosOcp()\n",
    "        ocp.model = model\n",
    "        ocp.dims.N = self.N\n",
    "        ocp.solver_options.tf = self.N * self.dt\n",
    "        ocp.solver_options.qp_solver = \"FULL_CONDENSING_HPIPM\"\n",
    "        ocp.solver_options.integrator_type = \"ERK\"\n",
    "        ocp.solver_options.nlp_solver_type = \"SQP\"\n",
    "\n",
    "        # Set up other hyperparameters in SQP solving\n",
    "        ocp.solver_options.nlp_solver_max_iter = 100\n",
    "        ocp.solver_options.nlp_solver_tol_stat = 1E-6\n",
    "        ocp.solver_options.nlp_solver_tol_eq = 1E-6\n",
    "        ocp.solver_options.nlp_solver_tol_ineq = 1E-6\n",
    "        ocp.solver_options.nlp_solver_tol_comp = 1E-6\n",
    "\n",
    "        ocp.cost.cost_type = \"LINEAR_LS\"\n",
    "        ocp.cost.cost_type_e = \"LINEAR_LS\"\n",
    "\n",
    "        ocp.cost.W = np.block([\n",
    "            [self.Q, np.zeros((self.dim_states, self.dim_inputs))],\n",
    "            [np.zeros((self.dim_inputs, self.dim_states)), self.R]\n",
    "        ])\n",
    "        ocp.cost.W_e = self.Qf\n",
    "\n",
    "        ocp.cost.Vx = np.block([\n",
    "            [np.eye(self.dim_states)],\n",
    "            [np.zeros((self.dim_inputs, self.dim_states))]\n",
    "        ])\n",
    "        ocp.cost.Vu = np.block([\n",
    "            [np.zeros((self.dim_states, self.dim_inputs))],\n",
    "            [np.eye(self.dim_inputs)]\n",
    "        ])\n",
    "        ocp.cost.Vx_e = np.eye(self.dim_states)\n",
    "\n",
    "        ocp.cost.yref = np.zeros(self.dim_states + self.dim_inputs)\n",
    "        ocp.cost.yref_e = np.zeros(self.dim_states)\n",
    "\n",
    "        # Define constraints\n",
    "        ocp.constraints.idxbx_0 = np.arange(self.dim_states)\n",
    "        ocp.constraints.idxbx = np.arange(self.dim_states)\n",
    "        ocp.constraints.idxbx_e = np.arange(self.dim_states)\n",
    "        ocp.constraints.idxbu = np.arange(self.dim_inputs)\n",
    "        \n",
    "        ocp.constraints.lbx_0 = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_0 = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbx_e = np.array(self.env.state_lbs)\n",
    "        ocp.constraints.ubx_e = np.array(self.env.state_ubs)\n",
    "        ocp.constraints.lbu = np.array(self.env.input_lbs)\n",
    "        ocp.constraints.ubu = np.array(self.env.input_ubs)\n",
    "\n",
    "        self.ocp = ocp\n",
    "        self.solver = AcadosOcpSolver(ocp, json_file=f\"{self.name}.json\")\n",
    "\n",
    "    def propagate_uncertainty(self, x, u, Sigma_curr):\n",
    "        Sigma_next = self.dynamics.dynamics_variance_function_disc(x, u) + self.Sigma_w\n",
    "        return Sigma_next\n",
    "\n",
    "    def tighten_state_constraints(self, Sigma_x):\n",
    "        r = self.beta\n",
    "        diag = np.sqrt(np.diag(Sigma_x))\n",
    "        if self.verbose:\n",
    "            print(f\"Tightening state constraints with beta={self.beta}, diag={diag}\")\n",
    "        return self.env.state_lbs + r * diag, self.env.state_ubs - r * diag\n",
    "\n",
    "    def compute_action(self, current_state: np.ndarray, current_time):\n",
    "        self.solver.set(0, \"lbx\", current_state)\n",
    "        self.solver.set(0, \"ubx\", current_state)\n",
    "\n",
    "        # === Step 1: Use previous rollout to predict mean state/control sequence ===\n",
    "        mu_x_seq = [current_state]\n",
    "        mu_u_seq = []\n",
    "\n",
    "        for i in range(self.N):\n",
    "            # Use last solved control prediction if available, otherwise use zeros\n",
    "            if hasattr(self, \"last_u_pred\"):\n",
    "                mu_u = self.last_u_pred[i]\n",
    "            else:\n",
    "                mu_u = np.zeros(self.dim_inputs)\n",
    "            mu_x = mu_x_seq[-1]\n",
    "\n",
    "            # Compute next state mean via dynamics\n",
    "            mu_x_next = self.dynamics.one_step_forward(mu_x, mu_u, self.dt)  # should match rollout model\n",
    "            mu_x_seq.append(mu_x_next)\n",
    "            mu_u_seq.append(mu_u)\n",
    "\n",
    "        # === Step 2: Propagate uncertainty along predicted trajectory ===\n",
    "        Sigma_x_seq = [np.zeros((self.dim_states, self.dim_states))]\n",
    "\n",
    "        for i in range(self.N):\n",
    "            Sigma_x_next = self.propagate_uncertainty(\n",
    "                mu_x_seq[i], mu_u_seq[i], Sigma_x_seq[i]\n",
    "            )\n",
    "            Sigma_x_seq.append(Sigma_x_next)\n",
    "\n",
    "        # === Step 3: Apply constraint tightening and set to Acados ===\n",
    "        ref = np.concatenate((self.env.target_state, np.zeros(self.dim_inputs)))\n",
    "        for i in range(1, self.N):\n",
    "            # Set reference\n",
    "            self.solver.set(i, \"yref\", ref)\n",
    "\n",
    "            # # Tighten constraints based on propagated covariance\n",
    "            lbx_tight, ubx_tight = self.tighten_state_constraints(Sigma_x_seq[i])\n",
    "            self.solver.set(i, \"lbx\", lbx_tight)\n",
    "            self.solver.set(i, \"ubx\", ubx_tight)\n",
    "            if self.verbose:\n",
    "                print(f\"Step {i}: lbx={lbx_tight}, ubx={ubx_tight}\")\n",
    "\n",
    "        self.solver.set(self.N, \"yref\", self.env.target_state)\n",
    "        lbx_tight, ubx_tight = self.tighten_state_constraints(Sigma_x_seq[self.N])\n",
    "        self.solver.set(self.N, \"lbx\", lbx_tight)\n",
    "        self.solver.set(self.N, \"ubx\", ubx_tight)\n",
    "        if self.verbose:\n",
    "            print(f\"Final step: lbx={lbx_tight}, ubx={ubx_tight}\")\n",
    "\n",
    "        # === Step 4: Solve MPC ===\n",
    "        status = self.solver.solve()\n",
    "        #if status != 0:\n",
    "        #    raise ValueError(f\"Acados solver failed with status {status}\")\n",
    "\n",
    "        u_opt = self.solver.get(0, \"u\")\n",
    "        x_pred = np.array([self.solver.get(i, \"x\") for i in range(self.N + 1)])\n",
    "        u_pred = np.array([self.solver.get(i, \"u\") for i in range(self.N)])\n",
    "\n",
    "        # Cache rollout for next time\n",
    "        self.last_u_pred = u_pred.copy()\n",
    "        self.last_x_pred = x_pred.copy()\n",
    "\n",
    "        return u_opt, x_pred, u_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad7aa9",
   "metadata": {},
   "source": [
    "We instantiate the learning-based MPC method developed above and evaluate its performance through simulation. The parameter settings used in the system identification phase are listed below.\n",
    "\n",
    "- **Reference Arguments for Basis function configuration:**\n",
    "\n",
    "    - Include offset: `True`\n",
    "\n",
    "    - Include polynomial terms: `False`\n",
    "\n",
    "    - Include trigonometric terms: `True`\n",
    "\n",
    "    - Minimum frequency: `16`\n",
    "\n",
    "    - Maximum frequency: `20`\n",
    "\n",
    "- **Reference Arguments for Data generation:**\n",
    "\n",
    "    - Number of samples: `1000`\n",
    "\n",
    "    - Measurement noise: zero-mean Gaussian, standard deviation $\\sigma$ as `0.005`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4188136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prior parameters\n",
    "sigma_prior = 0.005\n",
    "\n",
    "# Define basis functions\n",
    "include_offset=True\n",
    "include_polynomial=False\n",
    "include_trigonometric=True\n",
    "min_poly_order=1\n",
    "max_poly_order=1\n",
    "min_freq=16\n",
    "max_freq=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5903dd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training data\n",
    "np.random.seed(49) # for reproducibility\n",
    "data_gen = GenerateData(num_samples=1000, case=case_real, param=terrain_param)\n",
    "data_gen.set_noise(mean=0.0, std=sigma_prior) # (I.I.D.) zero-mean Gaussian noise\n",
    "p, h = data_gen.generate_data()\n",
    "\n",
    "\n",
    "# Generate basis functions\n",
    "basis, p_sym, basis_sym = generate_basis_functions(include_offset, include_polynomial, include_trigonometric, \n",
    "                                                   min_poly_order, max_poly_order, min_freq, max_freq)\n",
    "\n",
    "# Fit the model using linear regression\n",
    "model_blr = Identifier_BLR(basis, sigma2=sigma_prior**2)\n",
    "model_blr.fit(p, h)\n",
    "\n",
    "\n",
    "# Visualize the results\n",
    "true_func = data_gen.get_symbolic_function()\n",
    "model_blr.plot(true_func=true_func, title=\"Estimation Bias Caused by Measurement Noise in Linear Regression\")\n",
    "\n",
    "\n",
    "# Generate the symbolic expression for the learned model\n",
    "h_func_blr, sigma2_h_func_blr = construct_casadi_expression(p_sym, basis_sym, model_blr.theta, model_blr.Sigma_theta, sigma_prior**2)\n",
    "\n",
    "\n",
    "# Identified case for simulation\n",
    "env_id = Env(case_real, np.array([initial_position, initial_velocity]), np.array([target_position, target_velocity]),\n",
    "                 symbolic_h_mean_ext=h_func_blr,  symbolic_h_cov_ext=sigma2_h_func_blr, param=terrain_param,\n",
    "                 state_lbs=state_lbs, state_ubs=state_ubs, input_lbs=input_lbs, input_ubs=input_ubs)\n",
    "dynamics_id = Dynamics(env_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09effe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LBMPC controller\n",
    "controller_lbmpc = LearningBasedMPCController(env_id, dynamics_id, Q, R, Qf, N, freq, name='LBMPC')\n",
    "\n",
    "# Instantiate the simulator, run the simulation, and plot the results\n",
    "simulator_lbmpc = Simulator(dynamics_real, controller_lbmpc, env_real, 1/freq, t_terminal)\n",
    "simulator_lbmpc.run_simulation()\n",
    "\n",
    "# Instantiate the visualizer, and display the plottings and animation\n",
    "visualizer_lbmpc = Visualizer(simulator_lbmpc)\n",
    "visualizer_lbmpc.display_contrast_plots(simulator_rmpc, title=\"Performance Comparison: Learning-based MPC vs. RMPC with Flat Terrain Model\")\n",
    "visualizer_lbmpc.display_contrast_animation_same(simulator_rmpc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927272af",
   "metadata": {},
   "source": [
    "#### **Results Analysis**\n",
    "\n",
    "Compared to RMPC, LBMPC accurately reaches the target state, whereas RMPC only stabilizes within a neighborhood due to its conservative design. This performance gap arises because RMPC is based on a flat terrain model and fails to generate enough thrust to overcome the slope. In contrast, LBMPC captures the slope profile through Bayesian learning, enabling more informed predictions and tighter control actions. As a result, LBMPC achieves lower steady-state error and more efficient input usage under the same constraints.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb1d090",
   "metadata": {},
   "source": [
    "<blockquote style=\"padding-top: 20px; padding-bottom: 10px;\">\n",
    "\n",
    "##### **💡 Take-away:**\n",
    "\n",
    "- ✅ When the **true model is known**, classic model-based control performs optimally.  \n",
    "\n",
    "- ⚠️ In practice, **model mismatch is unavoidable**. Naively reusing old controllers or applying RMPC with rough bounds often leads to failure (e.g., steady-state errors). \n",
    "\n",
    "- 🔍 **Learning a model from data** is key:\n",
    "\n",
    "  - **Linear Regression (LR)** gives a deterministic approximation.\n",
    "\n",
    "  - **Bayesian Linear Regression (BLR)** further captures uncertainty for safer control.\n",
    "  \n",
    "- 🚀 With a properly identified model, especially probabilistic ones, **Learning-Based MPC (LBMPC)** can restore near-optimal performance and enable **constraint tightening** under uncertainty.\n",
    "\n",
    "</blockquote>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
